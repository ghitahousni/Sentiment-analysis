{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "351d30fa-312a-4295-83b9-b081d37952bb",
   "metadata": {},
   "source": [
    "# Theory and Applications of Data Analytics (LDSCI7236)\n",
    "\n",
    "### AE2 Sentiment \n",
    "\n",
    "#### Student ID: 23220052"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a62f8-286d-45a5-a468-bf0f00703641",
   "metadata": {},
   "source": [
    "## I - Exploratory data analysis (30 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd33836-4b1e-4dba-8ab8-52cdf26c4845",
   "metadata": {},
   "source": [
    "Before starting the assignment, we will be importing all libraries and packages required for our programme solution\n",
    "\n",
    "- \"import os\" module to use the operating system dependent functionality allowing us to interact with file systems, manipulate paths and perform operating system-related tasks. (Python documentation, 2024)\n",
    "- \"import urllib\" module to retrieve/fetch/workwith data from URLs (Python documentation, 2024)\n",
    "- \"import time\" module to work with time-related functions (Python documentation, 2024)\n",
    "- \"import csv\" module to work with Comma Separated Values files as well as read and write tabular data in CSV format (Python documentation, 2024)\n",
    "- \"import pandas as pd\" library to manipulate and analyse data (W3 School, 2024) \n",
    "- \"import numpy as np\" library to perform mathematical operations (W3 School, 2024) \n",
    "- \"import statistics\" module to calculate mathematical statistics of numeric data (Python documentation, 2024)\n",
    "- \"import seaborn as sns\" data visualization library (based on Matplotlib) to make statistical graphics, explore and understand the data (Seaborn documentation, 2024) \n",
    "- \"import matplotlib.pyplot as plt\" module to create different types of plots and charts (Matplotlib documentation, 2024)\n",
    "- \"import request\" module to allow us to send HTTP requests (SitePoint, 2024)\n",
    "- \"import zipfile\" module to create, read, write and list contents of ZIP archives/files (Real Python, 2023)\n",
    "- \"import re\" module to provide regular expression matching operations(Re, 2024) \n",
    "- \"import pickle\" module for serializing and deserializing Python objects (Python documentation, 2024)\n",
    "- \"from sklearn.feature_extraction.text import TfidfVectorizer\" to convert a collection of raw documents to a matrix of TF-IDF features (scikit, 2024)\n",
    "- \"from sklearn.linear_model import LogisticRegression\" to use Logistic Regression classifier (scikit, 2024)\n",
    "- \"from sklearn.model_selection import train_test_split\" to split arrays or matrices into random train and test subsets (scikit, 2024)\n",
    "- \"from sklearn.naive_bayes import MultinomialNB\" to use Naive Bayes classifier for multinomial models (scikit, 2024)\n",
    "- \"from sklearn.metrics import classification_report\" to build a text report showing the main classification metrics (scikit, 2024)\n",
    "- \"from sklearn.feature_extraction.text import CountVectorizer\" to convert a collection of text documents to a matrix of token counts (scikit, 2024)\n",
    "- \"from sklearn.metrics import accuracy_score\" for accuracy classification score (scikit, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "541e3b7f-a238-4326-ad4d-ac1936995d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We first import all libraries and packages \n",
    "\n",
    "import os \n",
    "import urllib\n",
    "import time \n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import zipfile\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3dba4-6e0c-4f29-8ad3-3943a52bfb19",
   "metadata": {},
   "source": [
    "To perform an exploratory data analysis (EDA) on the SST-2 dataset, we need to start by loading and examining the data from our files of interest : the train.tsv and dev.tsv files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65488fa2-bd8e-4e22-857a-62dd4bd2487f",
   "metadata": {},
   "source": [
    "#### a) Explore the distribution of positive and negative reviews in the SST-2 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66b94a-b4a4-445a-bbab-b9aa7cc978a0",
   "metadata": {},
   "source": [
    "In order to explore the distribution of positive and negative reviews in the SST-2 dataset, we develop a code to process the downloading, extracting, and preparing of SST-2 dataset for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb095adc-f8b0-4d8c-8021-7fd9f44d892e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST-2 dataset downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#URL of the SST-2 dataset\n",
    "sst_url = \"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n",
    "\n",
    "#We then send an HTTP GET request to download the SST-2 dataset\n",
    "response = requests.get(sst_url)\n",
    "\n",
    "#We check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    #We open a file in binary write mode to save the downloaded dataset\n",
    "    with open(\"SST-2.zip\", \"wb\") as f:\n",
    "        #We write the contents of the response to the file\n",
    "        f.write(response.content)\n",
    "    #We print message indicating successful download\n",
    "    print(\"SST-2 dataset downloaded successfully.\")\n",
    "else:\n",
    "    #Or a message indicating failure to download with status code\n",
    "    print(\"Failed to download SST-2 dataset. Status code:\", response.status_code)\n",
    "\n",
    "#We extract the contents of the downloaded ZIP file\n",
    "with zipfile.ZipFile(\"SST-2.zip\", \"r\") as zip_ref:\n",
    "    #We extract all contents to a directory named \"SST-2\"\n",
    "    zip_ref.extractall(\"SST-2\")\n",
    "\n",
    "#Finally, we define file paths for training and development sets\n",
    "train_path = os.path.join(\"SST-2\", \"SST-2\", \"train.tsv\")\n",
    "dev_path = os.path.join(\"SST-2\", \"SST-2\", \"dev.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296cd50-e4e3-49f8-9419-3de13fcaaddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code aims to download our dataset SST-2 (Stanford Sentiment Treebank 2) from a given URL, extract its contents, and then define file paths for its training and development sets. We first assign to our variable \"sst_url\" the URL where the SST-2 dataset is located. Then, using \"requests.get(sst_url)\", we send an HTTP GET request to our specified URL to retrieve the dataset. In a for loop, we check whether the HTTP request was successful (status code 200). If it is successful, the code proceeds to saving the dataset to a file named \"SST-2.zip\". Then, we ask the code to open the previous file in write mode. Please, note that \"f.write(response.content)\" writes the contents of the HTTP response to the file. Then, we ask the code to create a ZipFile object to work on the donwloaded ZIP file. All its content is extracted into a directory \"SST-2\" in our current notebook. Finally, we define the paths for our training and development set within the extracted dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2276d8-41f2-4669-9d7f-c86bf9a02a28",
   "metadata": {},
   "source": [
    "Then, we need to convert the tab-separated training and development datasets into CSV format and saves them as separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec639b3-ad39-40a3-9011-3b8dd3d9bf75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and dev datasets saved as CSV files successfully.\n"
     ]
    }
   ],
   "source": [
    "#We load train.tsv and dev.tsv into Dataframes\n",
    "\n",
    "#We first read the train.tsv file into a dataframe\n",
    "train_df = pd.read_csv(os.path.join(\"SST-2\", \"SST-2\", \"train.tsv\"), sep='\\t')\n",
    "\n",
    "#We then read the dev.tsv file into another dataframe\n",
    "dev_df = pd.read_csv(os.path.join(\"SST-2\", \"SST-2\", \"dev.tsv\"), sep='\\t')\n",
    "\n",
    "#We then want to save train.tsv and dev.tsv as CSV files\n",
    "\n",
    "#We save train_df as CSV file named \"train.csv\" \n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "\n",
    "#We save dev_df as CSV file named \"dev.csv\" \n",
    "dev_df.to_csv(\"dev.csv\", index=False)\n",
    "\n",
    "#We finally print a message confirming the successful saving of datasets as CSV files\n",
    "print(\"Train and dev datasets saved as CSV files successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53197534-0ba1-4d5f-a2c8-c517bd6159fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code aims to load the training and development datasets (train.tsv and dev.tsv, respectively) into dataframes from their respective file paths. Once done, it saves them as CSV files (train.csv and dev.csv, respectively). We first use \"pd.read_csv(os.path.join(\"SST-2\", \"SST-2\", \"train.tsv\"), sep='\\t')\" to read the train.tsv file into a dataframe, by creating the file path from the \"SST-2\", \"train.tsv\", and \"sep='\\t'\" which specifies that the data is tab-separated. We do the same for the dev.tsv file, so it reads it into another dataframe. Then, we save our train_df and dev_df respectively as separated CSV files. The \"index=False\" argument specifies that the index of the dataframes should not be saved to the CSV file. Finally, we print a message confirming that the datasets have been successfully saved as CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629a2d4-5c45-4379-b2bd-05f0ab71f056",
   "metadata": {},
   "source": [
    "Now we have our two dataframes as per the below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9770c2-4008-4d7e-b1b1-1d438975e1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90333308-6dee-4b27-870f-d629470afb60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it 's a charming and often affecting journey .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unflinchingly bleak and desperate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows us to hope that nolan is poised to emba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it 's slow -- very , very slow .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>has all the depth of a wading pool .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>a movie with a real anarchic flair .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>a subject like this should inspire reaction in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>... is an arthritic attempt at directing by ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>looking aristocratic , luminous yet careworn i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0      it 's a charming and often affecting journey .       1\n",
       "1                   unflinchingly bleak and desperate       0\n",
       "2    allows us to hope that nolan is poised to emba...      1\n",
       "3    the acting , costumes , music , cinematography...      1\n",
       "4                    it 's slow -- very , very slow .       0\n",
       "..                                                 ...    ...\n",
       "867              has all the depth of a wading pool .       0\n",
       "868              a movie with a real anarchic flair .       1\n",
       "869  a subject like this should inspire reaction in...      0\n",
       "870  ... is an arthritic attempt at directing by ca...      0\n",
       "871  looking aristocratic , luminous yet careworn i...      1\n",
       "\n",
       "[872 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadfeea-0957-41d0-b10e-79276e2f6212",
   "metadata": {},
   "source": [
    "Note 1: Upon reviewing our movie reviews dataset, we observed that the 'sentence' column contains punctuation, capital letters, and numeric values. Initially, we considered preprocessing and cleaning our data to address these elements. However, after careful consideration and examination of our CSV files, we determined that such preprocessing is unnecessary. Since our data will undergo tokenization during at a later phase, these elements would not significantly impact our results. Moreover, we also noticed the absence of missing labels and duplicates in our dataset. Therefore, we decided to proceed with the assignment without preprocessing the data, as we found no benefit to doing so. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abe5e6-a6a6-43d1-acd1-3e3e0cdafb8d",
   "metadata": {},
   "source": [
    "Now, from our above dataframes (train_df and dev_df), we will explore the positive and negative reviews for each dataframe respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeab7ed-c935-418c-b5ba-005a3462bebc",
   "metadata": {},
   "source": [
    "For train_df : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fffb0d5-7c64-485a-bde6-90c6547974a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 37569\n",
      "Number of negative reviews: 29780\n"
     ]
    }
   ],
   "source": [
    "#We calculate the count of each unique value in the 'label' column\n",
    "sentiment_counts_train = train_df['label'].value_counts()\n",
    "\n",
    "#We plot the distribution of sentiment labels\n",
    "#We set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "#We create a bar plot of sentiment label counts, with colors representing negative and positive sentiments\n",
    "sentiment_counts_train.plot(kind='bar', color=['red', 'green'])\n",
    "#We set plot title, x-axis label, and y-axis label\n",
    "plt.title('Figure 1: Distribution of Sentiment Labels in Train Dataset')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "#Then, we set x-axis tick labels and their positions\n",
    "plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'], rotation=0)\n",
    "#Finally, we show the plot\n",
    "plt.show()\n",
    "\n",
    "#Then, we print the count of positive and negative reviews\n",
    "#We access the count of positive sentiment reviews and print\n",
    "print(\"Number of positive reviews:\", sentiment_counts_train[1])\n",
    "#Then, we access the count of negative sentiment reviews and print\n",
    "print(\"Number of negative reviews:\", sentiment_counts_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630e3ac-d382-4b19-a366-626589f33cc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code performs data exploration and visualization on our training dataset (train_df). By using the \"train_df['label'].value_counts()\", this code calculates the count of each unique value in the 'label' column of the train_df dataframe. It provides insight into the distribution of sentiment labels (presumably positive (1) and negative (0)) in our dataset. Then, we ask the code to create a bar plot where sentiment labels are represented on the x-axis and their corresponding counts are represented on the y-axis. The colors red and green correspond to negative and positive sentiments, respectively. Finally, we ask the code to print the count of positive and negative sentiment reviews, respectively, based on the calculated sentiment counts stored in the sentiment_counts_train variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c93bde-8d75-49aa-b070-b1f4d8861757",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Discussion: \n",
    "\n",
    "From the above illustration, we notice that there are 37569 positive reviews and 29780 negative reviews. This means that, 37569 sentences or reviews are labelled as 1 (positive) while 29780 are labelled as 0 (negative). The above information provides insight into the distribution of sentiment labels in our training dataset. It suggests that there are more positive reviews than negative ones. Therefore, we can conclude that the movie had a more positive reception. \n",
    "\n",
    "However, we notice that there is an unequal distribution of positive and negative reviews which can suggest an imbalance in the training dataset. This can ulterly impact the training and performance of the model in a sentiment analysis task. Therefore, it is crucial to account for this disparity during model evaluation and selection to ensure the reliability and effectiveness of sentiment analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035945d3-b146-4f24-bea8-6702f6c31fb3",
   "metadata": {},
   "source": [
    "Now, we do exactly the same for dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc291050-eded-4ee5-9665-9d38f68d1ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 444\n",
      "Number of negative reviews: 428\n"
     ]
    }
   ],
   "source": [
    "#We calculate the count of each unique value in the 'label' column\n",
    "sentiment_counts_dev = dev_df['label'].value_counts()\n",
    "\n",
    "#We plot the distribution of sentiment labels\n",
    "#We set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "#We create a bar plot of sentiment label counts, with colors representing negative and positive sentiments\n",
    "sentiment_counts_dev.plot(kind='bar', color=['red', 'green'])\n",
    "#We set plot title, x-axis label, and y-axis label\n",
    "plt.title('Figure 2: Distribution of Sentiment Labels in Dev Dataset')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "#Then, we set x-axis tick labels and their positions\n",
    "plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'], rotation=0)\n",
    "#Finally, we show the plot\n",
    "plt.show()\n",
    "\n",
    "#Then, we print the count of positive and negative reviews\n",
    "#We access the count of positive sentiment reviews and print\n",
    "print(\"Number of positive reviews:\", sentiment_counts_dev[1])\n",
    "#Then, we access the count of negative sentiment reviews and print\n",
    "print(\"Number of negative reviews:\", sentiment_counts_dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3d2e3-d2f8-44c6-9843-dbeb3b2490b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code performs data exploration and visualization on our development dataset (dev_df). By using the \"dev_df['label'].value_counts()\", this code calculates the count of each unique value in the 'label' column of the dev_df dataframe. It provides insight into the distribution of sentiment labels (presumably positive (1) and negative (0)) in our dataset. Then, we ask the code to create a bar plot where sentiment labels are represented on the x-axis and their corresponding counts are represented on the y-axis. The colors red and green correspond to negative and positive sentiments, respectively. Finally, we ask the code to print the count of positive and negative sentiment reviews, respectively, based on the calculated sentiment counts stored in the sentiment_counts_dev variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30685de-2794-46c3-91a3-22dcff8b9307",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Discussion: \n",
    "\n",
    "From the above illustration, we notice that there are 444 positive reviews and 428 negative reviews. This means that, 444 sentences or reviews are labelled as 1 (positive) while 428 are labelled as 0 (negative). The above information provides insight into the distribution of sentiment labels in our development dataset. It suggests that there are more positive reviews than negative ones. Therefore, we can conclude that the movie had a more positive reception, which aligns with the outcome of our training dataset. \n",
    "\n",
    "However, we notice that there is an unequal distribution of positive and negative reviews which can suggest an imbalance in the training dataset. This can ulterly impact the training and performance of the model in a sentiment analysis task. Therefore, it is crucial to account for this disparity during model evaluation and selection to ensure the reliability and effectiveness of sentiment analysis results.\n",
    "\n",
    "Finally, we notice that there is a difference in the count of positive and negative reviews for both datasets. This can be explained by the size of datasets. Indeed, our training dataset is larger than our development dataset which explains the smaller count of positive and negative reviews in our development dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ef04d-3933-42d6-92fd-4dc661c1d7c5",
   "metadata": {},
   "source": [
    "#### b) Identify any data imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45215f54-2782-4f3b-942f-4dbcf3ed2a87",
   "metadata": {},
   "source": [
    "As per the above, we noticed that there is an imbalance in the size of our datasets as well as in their size of positive and negative reviews respectively. In this section, we aim to identify these data imbalances to get insights into the distribution of labels across both the training and development datasets, in order to understand the balance or imbalance of classes within each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdde302-ae8c-4a58-8fef-28326eee0a0f",
   "metadata": {},
   "source": [
    "We first want to check the imbalances and differences between the datasets. To do that, we calculate the counts of positive and negative entries in both the training and development datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2dafdf-8cbd-41e3-96bc-b8eb8f0c868d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Positive Entries: 37569\n",
      "Negative Entries: 29780\n",
      "Difference: 7789\n",
      "\n",
      "Development Data:\n",
      "Positive Entries: 444\n",
      "Negative Entries: 428\n",
      "Difference: 16\n"
     ]
    }
   ],
   "source": [
    "#We want to count the number of positive and negative entries in the training and development datasets\n",
    "\n",
    "#We first calculate the number of entries where 'label' column equals 1 (positive sentiment) and 0 (negative sentiment) in the training dataset\n",
    "train_positive_count = (train_df['label'] == 1).sum()\n",
    "train_negative_count = (train_df['label'] == 0).sum()\n",
    "\n",
    "#We then calculate the number of entries where 'label' column equals 1 (positive sentiment) and 0 (negative sentiment) in the development dataset\n",
    "dev_positive_count = (dev_df['label'] == 1).sum()\n",
    "dev_negative_count = (dev_df['label'] == 0).sum()\n",
    "\n",
    "#We print the counts of positive and negative entries in the training dataset\n",
    "print(\"Training Data:\")\n",
    "print(\"Positive Entries:\", train_positive_count)\n",
    "print(\"Negative Entries:\", train_negative_count)\n",
    "#And calculate and print the absolute difference between the counts of positive and negative entries in the training dataset\n",
    "print(\"Difference:\", abs(train_positive_count - train_negative_count))\n",
    "\n",
    "#We do the development dataset\n",
    "print(\"\\nDevelopment Data:\")\n",
    "print(\"Positive Entries:\", dev_positive_count)\n",
    "print(\"Negative Entries:\", dev_negative_count)\n",
    "\n",
    "print(\"Difference:\", abs(dev_positive_count - dev_negative_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c4d1c-e278-4880-bbea-b99e2b6aabde",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code calculate the counts of positive and negative entries in both the training and development datasets. It assumes that the datasets are already loaded and that the 'label' column contains sentiment labels, with 0 representing negative sentiment and 1 representing positive sentiment. Indeed, we first calculate the number of entries in the training dataset where the 'label' column equals 1, indicating positive sentiment (stored in variable train_positive_count). It sums up all the True values resulting from the comparison. Then, we calculate the number of entries in the training dataset where the 'label' column equals 0, indicating negative sentiment (stored in train_negative_count variable). We then do the same for development dataset, where the number of positive entries is tored in dev_positive_count variable and the number of negative entries is stored in dev_negative_count variables. We finally print the results, along with the absolute difference between the counts for both training and development dataset. \n",
    "\n",
    "This way, we are able to understand the dataset characteristics before proceeding with sentiment analysis tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3829543-2b74-4019-a8fd-15d2caae4aa3",
   "metadata": {},
   "source": [
    "Then, we want to generate a bar plot that visually compares the counts of positive and negative entries between the training and development datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134f18d9-c9af-45b1-ae06-3f130251d8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We first define labels for the x-axis categories\n",
    "train_labels = ['Positive', 'Negative']\n",
    "dev_labels = ['Positive', 'Negative']\n",
    "\n",
    "#As well as the counts of positive and negative entries in the training and development datasets\n",
    "train_counts = [train_positive_count, train_negative_count]\n",
    "dev_counts = [dev_positive_count, dev_negative_count]\n",
    "\n",
    "#We then create a figure and axis object for the plot with a specified size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "#And generate an index range for positioning the bars on the x-axis\n",
    "index = range(len(train_labels))\n",
    "\n",
    "#We then plot bars representing counts of positive and negative entries in the training dataset\n",
    "train_bars = ax.bar(index, train_counts, bar_width, label='Train Data')\n",
    "#As well as plot bars representing counts of positive and negative entries in the development dataset\n",
    "dev_bars = ax.bar([i + bar_width for i in index], dev_counts, bar_width, label='Dev Data')\n",
    "\n",
    "#We set labels for the x-axis and y-axis\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "#We set the title of the plot\n",
    "ax.set_title('Figure 3: Counts of Positive and Negative Entries in Train and Dev Data')\n",
    "#We set positions and labels for the ticks on the x-axis\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(train_labels)\n",
    "#We display the legend indicating the bars representing the training and development datasets\n",
    "ax.legend()\n",
    "\n",
    "#We add counts above bars\n",
    "for bars in [train_bars, dev_bars]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        # Add text annotations above each bar to display its count\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "#Then adjust subplot parameters to prevent overlapping\n",
    "plt.tight_layout()\n",
    "#Finally we display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf2bf7-a886-43a1-b8ea-8caf3e8e829c",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "    \n",
    "The above code generates a bar plot to visualize the counts of positive and negative entries in both the training and development datasets. To do that, we first give the code train_labels and dev_labels lists that contain labels for the x-axis categories, which are 'Positive' and 'Negative' in this case. Then, we provide train_counts and dev_counts lists that contain the counts of positive and negative entries in the training and development datasets, respectively. We then create a bar plot positioning the bars on the x-axis and plotting the bars representing the counts of positive and negative entries in the training and development dataset. Finally, we set labels for the x-axis and y-axis, respectively as well as the title of the plot, and a legend indicating the bars representing the training and development datasets. We also ask the code to iterate through the bars and adds text annotations above each bar to display its counts. Finally we adjust subplot parameters to prevent overlapping and display the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8307eb-fe38-4dc6-a63d-eab708a32e99",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "    \n",
    "From the illustration above, for the training dataset, we see that there are 37569 reviews classified as positive sentiment and 29780 reviews classified as negative sentiment. However, the absolute difference between the counts of positive and negative entries is 7789. This suggests an imbalance in the distribution of sentiment labels within the training dataset, with a higher proportion of positive sentiment instances compared to negative sentiment instances. This imbalance could potentially affect the performance of models trained on this dataset, particularly in scenarios where balanced representation of classes is important for accurate predictions. This correspond to our previous analysis.\n",
    "\n",
    "For the development data, we notice that there are 444 reviews classified as positive sentiment and 428 reviews classified as negative sentiment. The absolute difference between the counts of positive and negative entries is 16. Therefore, unlike the training dataset, the development dataset exhibits a relatively balanced distribution between positive and negative sentiment instances, with a minimal difference of 16 entries. This balanced distribution could potentially lead to more robust model evaluation and performance assessment on unseen data, as the model's predictions are tested on a dataset that is more representative of real-world sentiment distributions. This correspond to our previous analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b088a-6c3e-41a2-a9de-b1900996e73d",
   "metadata": {},
   "source": [
    "Then, we check the distribution of labels in our train dataset then in our development dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab98715a-dea4-41c7-a5f8-d3eb1df50c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Distribution (Percentages):\n",
      "label\n",
      "1    55.782565\n",
      "0    44.217435\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Dev Label Distribution (Percentages):\n",
      "label\n",
      "1    50.917431\n",
      "0    49.082569\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We calculate the distribution of unique labels in the 'label' column of our training dataset,\n",
    "#and normalize the counts to percentages\n",
    "train_label_distribution = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "#We do the same for our development dataset\n",
    "dev_label_distribution = dev_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "#We then print a header indicating the label distribution in our training dataset\n",
    "print(\"Train Label Distribution (Percentages):\")\n",
    "#Then we print the calculated label distribution in our training dataset\n",
    "print(train_label_distribution)\n",
    "\n",
    "#We finally do the same for our development dataset\n",
    "print(\"\\nDev Label Distribution (Percentages):\")\n",
    "print(dev_label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da618013-bd8a-422d-b022-c9f421a1ebb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "Our above code aims to analyze the distribution of labels within our training and development dataset (train_df and dev_df, respectively). To do that, we ask the code to calculate the distribution of unique labels in the 'label' column of our training dataset (train_df). By setting normalize=True, the resulting counts are converted into percentages, representing the proportion of each label relative to the total number of instances in the dataset. We then do the same for our development dataset (dev_df). We then ask the code to print a header indicating the output that represtns the label distribution in the training and development dataset, as well as the calculated label distribution in the dataset respectively. Each unique label is displayed along with its corresponding percentage of occurrences. The \"\\n\" is a newline character, ensuring that there is a blank line between the previous and following outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831034fb-911d-41ba-83c9-710b1425205d",
   "metadata": {},
   "source": [
    "Then we create a figure with two subplots, each displaying a bar chart of the label distribution for the training and development datasets, respectively, providing a visual comparison of the label proportions between the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556af213-f771-4f0f-b066-abb1968794fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We want to plot each dataset distribution for better visualization\n",
    "\n",
    "#We create a figure and subplots layout with 1 row and 2 columns, and specify the figure size\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "#We plot the label distribution for the training dataset on the first subplot\n",
    "train_label_distribution.plot(kind='bar', ax=ax[0], color='skyblue')\n",
    "#We then set title, x-axis label, y-axis label, and grid for the first subplot\n",
    "ax[0].set_title('Figure 4: Train Label Distribution')\n",
    "ax[0].set_xlabel('Label')\n",
    "ax[0].set_ylabel('Proportion')\n",
    "ax[0].grid(axis='y')\n",
    "\n",
    "#Secondly, we plot the label distribution for the development dataset on the second subplot\n",
    "dev_label_distribution.plot(kind='bar', ax=ax[1], color='lightgreen')\n",
    "#We then set title, x-axis label, y-axis label, and grid for the second subplot\n",
    "ax[1].set_title('Firgure 5: Dev Label Distribution')\n",
    "ax[1].set_xlabel('Label')\n",
    "ax[1].set_ylabel('Proportion')\n",
    "ax[1].grid(axis='y')\n",
    "\n",
    "#Finally, we adjust subplot spacing to prevent overlap\n",
    "plt.tight_layout()\n",
    "#And display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95e702-af02-44d0-b5ba-b6a2da884392",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "In the above code, we aim to visualize the distributions of label proportions for both the training and development datasets. We ask the code to provide us with a bar chart of the label distribution for the training dataset on the first subplot (ax[0]), we set a title to it, as well as the label for the x-axis and y-axis of the first subplot. With the \"ax[0].grid(axis='y')\", we ask the code to add gridlines along the y-axis of the first subplot to aid visualization. Then, we do the same for our development dataset, defined as the second subplot (ax[1]). We finally adjust the spacing between subplots to prevent overlap and display the entire figure containing both subplots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cb66be-64d6-4bf3-a9fc-20e69bf9dfbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Discussion: \n",
    "\n",
    "From the above illustration, we notice that the majority of reviews in the training dataset are labeled as positive, constituting approximately 55.78% of the total dataset. On the other hand, the negative reviews make up the remaining proportion, accounting for approximately 44.22% of the dataset. Therefore, we conclude that there is a noticeable imbalance between the positive and negative labels (11.56%) in the training data, with the positive class being slightly overrepresented. This imbalance might be due to the higher amount of reviews, potentially leading to less accurate representation of sentiment proportions. In other words, this imbalance may potentially affect the model's learning process and performance, particularly in scenarios where the negative reviews are of interest.\n",
    "\n",
    "Regarding the development dataset, the proportions are more balanced between the positive and negative labels. Indeed, the positive reviews make up approximately 50.92% of the dataset, while the negative ones account for the remaining proportion, approximately 49.08% of the dataset. Therefore, we conclude that the development dataset has a more balanced distribution of labels (1.84%). This more balanced distribution in the development dataset might be indicative of a more accurate representation of sentiment proportions. In other words, the development dataset shows a more balanced distribution between positive and negative reviews, which might indicate a better representation of real-world data.\n",
    "\n",
    "\n",
    "It is important to note that the development dataset is typically a sample of the training dataset. While the training dataset would need more data for the model to train on and be more accurate, the development dataset is more suited for running the model's performance once it is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a3cb3-a26e-4029-82ca-80a9b29298f1",
   "metadata": {},
   "source": [
    "#### c) Discuss potential implications for model training and evaluation.\n",
    "\n",
    "As previously discussed, when identifying data imbalances in a dataset, particularly in the context of model training and evaluation for tasks like sentiment analysis, it is essential to consider several potential implications.\n",
    "\n",
    "Indeed, imbalanced datasets can lead to biased models, where the model may be more inclined to predict the majority class. For instance, our training dataset contains significantly more positive reviews than negative ones, a model trained on this data may perform well on positive reviews but poorly on negative ones. This can result in skewed evaluation metrics, such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Moreover, imbalances make it harder for the model to learn from minority classes since there are fewer examples available for training. As a result, the model may struggle to correctly classify instances belonging to minority classes, leading to lower performance metrics for those classes.\n",
    "\n",
    "Additionally, biased models trained on imbalanced datasets may not generalize well to real-world scenarios where class distributions differ. For instance, a sentiment analysis model trained on a dataset heavily skewed towards positive reviews such as our training dataset, may fail to accurately classify reviews in domains where negative sentiment is prevalent.\n",
    "\n",
    "Finally, when evaluating model performance on imbalanced datasets, it is crucial to use appropriate evaluation metrics that consider class imbalances. Metrics like accuracy can be misleading, especially when the classes are heavily imbalanced. Instead, using metrics such as precision, recall and F1 score provide a more comprehensive understanding of model performance across different classes.\n",
    "\n",
    "To address data imbalances during model training, it is crucial to oversample the minority class or undersampling the majority class to balance the dataset. Another method would be assigning higher weights to minority class samples during model training to give them more importance or generate synthetic data points for minority classes to increase their representation. \n",
    "\n",
    "To conclude, biased models trained on imbalanced data can perpetuate existing biases present in the data. Therefore, it is crucial to consider the ethical implications of deploying such models, especially in sentiment analysis, where biased predictions can impact user experiences and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b90c1-fe43-44b7-92e7-9fb96e0f8900",
   "metadata": {},
   "source": [
    "#### d) Apply appropriate visualisation of the data sets where necessary. \n",
    "\n",
    "Throughout the previous tasks, we have generated appropriate visualizations to help in the analysis and understanding of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03403512-62ae-4084-8934-355b07fcb832",
   "metadata": {},
   "source": [
    "## II - The text vectorisation method of choice (20 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411997b-03c5-43df-b91c-ecc8c954cbb0",
   "metadata": {},
   "source": [
    "#### a) Compare and contrast the efficiency of at least two vector representations of movie reviews in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2198b2-ac5e-4c92-962c-34e174c67130",
   "metadata": {},
   "source": [
    "For this question, we will be comparing and contrasting the efficiency of CountVectorizer and TF-IDF vector representations for movie reviews in the dataset. \n",
    "\n",
    "First, we will examine each method individually before making a final comparison and recommendation.\n",
    "\n",
    "\n",
    "##### CountVectorizer:\n",
    "\n",
    "CountVectorizer is a simple and commonly used technique for text representation in natural language processing (NLPs) which specifically deals with converting a collection of text into a numerical matrix (Bengfort, B. et al, nd). This method can be used as input for various machine learning algorithms tasks such as text classification, sentiment analysis, and information retrieval. This method consists of multiple steps (GfG,2022):\n",
    "\n",
    "- Tokenization which allows us to split our sentences (movie reviews) into individual words or tokens that can be easily analyzed. Please, note that punctuation, numbers, or special characters might be treated differently depending on our setup.\n",
    "\n",
    "- Vocabulary Building which enables us to create a vocabulary containing all unique tokens that appear in our sentences (movie reviews). This way, our unique tokens will be converted and mapped into a numerical ID.\n",
    "\n",
    "- Counting Occurrences which consists in counting how many times each word appears in each sentence (movie reviews). So for each word, it finds its corresponding ID in the vocabulary, creates a sparse matrix where rows represent sentences (movie reviews) and columns represent our unique words from the vocabulary. The value in each cell represents the count of how many times that particular term appeared in the corresponding sentence.\n",
    "\n",
    "This representation is straightforward and computationally efficient as they are quick to implement and suitable for large datasets. It can be applied to different types of text data and the result is easily interpretable as each dimension corresponds to a unique word in the vocabulary. Indeed, it involves creating a sparse matrix where each row corresponds to a document (movie review) and each column corresponds to a unique word in the corpus, and the value of each dimension represents the frequency of the corresponding word in the document. However, it represents a document as a collection of words (vocabulary) without considering the order in which they appear, focusing only on their frequency of occurrence within the document. Therefore, as it disregards word order and semantics, treating each word independently, it may result in high-dimensional sparse vectors which can lead to increased memory and computational requirements, as well as potential issues with data sparsity; and struggle to capture semantic relationships between words, limiting the model's ability to understand context and nuances in language (Ganesan, K., 2020).\n",
    "\n",
    "As discussed earlier, prior to using the dataset, we thought it might be interesting to remove digits as they might not be relevant to our task, hence allowing the simplification of the text and potentially improving model performance. It would also be interesting to lowercase the text to help standardize the data and reduce the vocabulary size, making it easier for the model to learn patterns. Finally, we can keep or remove stop words. Indeed, as they are commonly occurring words that often do not carry significant meaning for many tasks, keeping them can preserve contextual information and sentence structure, but removing them can reduce noise in the data and focus attention on more informative words. However, by using this CountVectorizer method, we would not need to preprocess our data as this technique can handle some basic text cleaning during the conversion process such as removing stop words (common words like \"the\", \"a\", \"an\"), converting all text to lowercase, and removing numerical data. \n",
    "\n",
    "\n",
    "##### TF-IDF Vectorization:\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a technique used to represent text documents based on the importance of terms in the document relative to their frequency across the entire corpus. This method combines the term frequency (TF), which measures how often a term appears in a document, with the inverse document frequency (IDF), which measures the rarity of a term across all documents in the corpus (Hyperskill, n.d). As TF-IDF assigns higher weights to terms that are frequent in the document but rare in the corpus, it captures their importance in representing the document. For instance, this method downweights stopwords that appear frequently across documents but carry little semantic meaning. Indeed, it reduces noise in the data and focuses attention on more informative words. Therefore, it can be used as input for various machine learning algorithms for tasks such as text classification, information retrieval, and document clustering (Otten, 2023). This method consists of multiple steps (Person, 2021):\n",
    "\n",
    "- Tokenization in order to break down each document (movie review) into individual words, which also involves removing punctuation and lowercasing the text.\n",
    "\n",
    "- Term Frequency (TF) Calculation to calculate the frequency of each term (word) in each document. This represents how often each term appears in the document.\n",
    "\n",
    "- Inverse Document Frequency (IDF) Calculation to calculate the inverse document frequency (IDF) for each term in the entire corpus. It measures how rare or common a term is across all documents in the corpus.\n",
    "\n",
    "- TF-IDF Calculation which consists in multiplying the term frequency (TF) of each term in a document by its inverse document frequency (IDF) to obtain the TF-IDF score. The latter combines the local importance of a term within a document (TF) with its global importance across the entire corpus (IDF).\n",
    "\n",
    "- Vectorization to represent each document as a vector where each dimension corresponds to a term (word) in the vocabulary, and the value represents the TF-IDF score of that term in the document. If a term is not present in a document, its TF-IDF score is typically set to zero.\n",
    "\n",
    "However, similarly to CounterVectorizer method, TF-IDF does not capture word order or semantic relationships between words, as it treats each term independently. This may limit its effectiveness in capturing contextual information and nuances in language, especially for tasks where context is crucial. Additionally, the size of the vocabulary in TF-IDF representations can be large, especially for datasets with diverse vocabulary. This can pose challenges in managing and processing large vocabularies, as well as in handling out-of-vocabulary words during inference. Finally, TF-IDF vectors often result in sparse representations, where most entries are zero due to the absence of terms in documents. While this sparsity helps in reducing memory usage, it may pose challenges for certain machine learning algorithms that are not well-suited to handle sparse data (Ninja, 2024).\n",
    "\n",
    "\n",
    "##### Comparison:\n",
    "\n",
    "In terms of efficiency, both vector representations are efficient in regards to implementation and computational resources. However, TF-IDF vectorization builds upon CountVectorizer method by weighting term frequencies with inverse document frequencies. This results in lower-dimensional representations compared to CounterVectorizer, especially for large vocabularies or datasets with many unique words. This can reduce memory and computational requirements, as well as potential issues with data sparsity.\n",
    "\n",
    "In terms of effectiveness, TF-IDF generally outperforms CountVectorizer method in capturing term importance and relevance. Indeed, while CountVectorizer method is simpler and quicker to implement, TF-IDF provides more nuanced representations by considering term frequencies across the corpus. It addresses some limitations of CountVectorizer method by emphasizing rare, discriminative terms and downweighting common words, which can be very useful while dealing with larger datasets or when the importance of terms needs to be emphasized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022a943-61ab-4fec-a142-5dd493ecbd76",
   "metadata": {},
   "source": [
    "Now we will use the CounterVectorizer representation method for our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0297b56f-1b2f-4b2f-afb2-09cec4a1fa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer DataFrame:\n",
      "                                                    000  10  100  101  103  \\\n",
      "hide new secretions from the parental units           0   0    0    0    0   \n",
      "contains no wit , only labored gags                   0   0    0    0    0   \n",
      "that loves its characters and communicates some...    0   0    0    0    0   \n",
      "remains utterly satisfied to remain the same th...    0   0    0    0    0   \n",
      "on the worst revenge-of-the-nerds clichés the f...    0   0    0    0    0   \n",
      "\n",
      "                                                    105  10th  11  110  112  \\\n",
      "hide new secretions from the parental units           0     0   0    0    0   \n",
      "contains no wit , only labored gags                   0     0   0    0    0   \n",
      "that loves its characters and communicates some...    0     0   0    0    0   \n",
      "remains utterly satisfied to remain the same th...    0     0   0    0    0   \n",
      "on the worst revenge-of-the-nerds clichés the f...    0     0   0    0    0   \n",
      "\n",
      "                                                    ...  zishe  ziyi  zoe  \\\n",
      "hide new secretions from the parental units         ...      0     0    0   \n",
      "contains no wit , only labored gags                 ...      0     0    0   \n",
      "that loves its characters and communicates some...  ...      0     0    0   \n",
      "remains utterly satisfied to remain the same th...  ...      0     0    0   \n",
      "on the worst revenge-of-the-nerds clichés the f...  ...      0     0    0   \n",
      "\n",
      "                                                    zombie  zone  zoning  \\\n",
      "hide new secretions from the parental units              0     0       0   \n",
      "contains no wit , only labored gags                      0     0       0   \n",
      "that loves its characters and communicates some...       0     0       0   \n",
      "remains utterly satisfied to remain the same th...       0     0       0   \n",
      "on the worst revenge-of-the-nerds clichés the f...       0     0       0   \n",
      "\n",
      "                                                    zoom  zwick  zzzzzzzzz  \\\n",
      "hide new secretions from the parental units            0      0          0   \n",
      "contains no wit , only labored gags                    0      0          0   \n",
      "that loves its characters and communicates some...     0      0          0   \n",
      "remains utterly satisfied to remain the same th...     0      0          0   \n",
      "on the worst revenge-of-the-nerds clichés the f...     0      0          0   \n",
      "\n",
      "                                                    élan  \n",
      "hide new secretions from the parental units            0  \n",
      "contains no wit , only labored gags                    0  \n",
      "that loves its characters and communicates some...     0  \n",
      "remains utterly satisfied to remain the same th...     0  \n",
      "on the worst revenge-of-the-nerds clichés the f...     0  \n",
      "\n",
      "[5 rows x 13774 columns]\n",
      "\n",
      "Vocabulary DataFrame:\n",
      "  word\n",
      "0  000\n",
      "1   10\n",
      "2  100\n",
      "3  101\n",
      "4  103\n"
     ]
    }
   ],
   "source": [
    "#We assume 'sentence' column contains the text of movie reviews in train_df\n",
    "sentence_data = train_df['sentence'].tolist()\n",
    "\n",
    "#We create a CountVectorizer instance\n",
    "vectorizer_cv = CountVectorizer(lowercase=True) # stop_words='english')\n",
    "\n",
    "#We fit the vectorizer on the sentence data and transform the sentence data into CountVectorizer representation\n",
    "vectorizer_train = vectorizer_cv.fit_transform(sentence_data)\n",
    "\n",
    "#We convert CountVectorizer representation to a dataframe for visualization\n",
    "countvectorizer_df = pd.DataFrame(vectorizer_train.toarray(), columns=vectorizer_cv.get_feature_names_out())\n",
    "\n",
    "#We set the index of the dataframe to our \"movie reviews\" (sentences)\n",
    "countvectorizer_df.index = sentence_data\n",
    "\n",
    "#We create a dataframe for the vocabulary\n",
    "bow_df = pd.DataFrame({'word': vectorizer_cv.get_feature_names_out()})\n",
    "\n",
    "#Finally, we display the first few rows of the CounterVectorizer DataFrame\n",
    "print(\"CountVectorizer DataFrame:\")\n",
    "print(countvectorizer_df.head())\n",
    "\n",
    "#As well as the first few rows of the Vocabulary DataFrame\n",
    "print(\"\\nVocabulary DataFrame:\")\n",
    "print(bow_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f22a571-6748-46ee-af5f-146b84013a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>103</th>\n",
       "      <th>105</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>112</th>\n",
       "      <th>...</th>\n",
       "      <th>zishe</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>élan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hide new secretions from the parental units</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains no wit , only labored gags</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that loves its characters and communicates something rather beautiful about human nature</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remains utterly satisfied to remain the same throughout</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a delightful comedy</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anguish , anger and frustration</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at achieving the modest , crowd-pleasing goals it sets for itself</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a patient viewer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this new jangle of noise , mayhem and stupidity must be a serious contender for the title .</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 13774 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    000  10  100  101  103  \\\n",
       "hide new secretions from the parental units           0   0    0    0    0   \n",
       "contains no wit , only labored gags                   0   0    0    0    0   \n",
       "that loves its characters and communicates some...    0   0    0    0    0   \n",
       "remains utterly satisfied to remain the same th...    0   0    0    0    0   \n",
       "on the worst revenge-of-the-nerds clichés the f...    0   0    0    0    0   \n",
       "...                                                 ...  ..  ...  ...  ...   \n",
       "a delightful comedy                                   0   0    0    0    0   \n",
       "anguish , anger and frustration                       0   0    0    0    0   \n",
       "at achieving the modest , crowd-pleasing goals ...    0   0    0    0    0   \n",
       "a patient viewer                                      0   0    0    0    0   \n",
       "this new jangle of noise , mayhem and stupidity...    0   0    0    0    0   \n",
       "\n",
       "                                                    105  10th  11  110  112  \\\n",
       "hide new secretions from the parental units           0     0   0    0    0   \n",
       "contains no wit , only labored gags                   0     0   0    0    0   \n",
       "that loves its characters and communicates some...    0     0   0    0    0   \n",
       "remains utterly satisfied to remain the same th...    0     0   0    0    0   \n",
       "on the worst revenge-of-the-nerds clichés the f...    0     0   0    0    0   \n",
       "...                                                 ...   ...  ..  ...  ...   \n",
       "a delightful comedy                                   0     0   0    0    0   \n",
       "anguish , anger and frustration                       0     0   0    0    0   \n",
       "at achieving the modest , crowd-pleasing goals ...    0     0   0    0    0   \n",
       "a patient viewer                                      0     0   0    0    0   \n",
       "this new jangle of noise , mayhem and stupidity...    0     0   0    0    0   \n",
       "\n",
       "                                                    ...  zishe  ziyi  zoe  \\\n",
       "hide new secretions from the parental units         ...      0     0    0   \n",
       "contains no wit , only labored gags                 ...      0     0    0   \n",
       "that loves its characters and communicates some...  ...      0     0    0   \n",
       "remains utterly satisfied to remain the same th...  ...      0     0    0   \n",
       "on the worst revenge-of-the-nerds clichés the f...  ...      0     0    0   \n",
       "...                                                 ...    ...   ...  ...   \n",
       "a delightful comedy                                 ...      0     0    0   \n",
       "anguish , anger and frustration                     ...      0     0    0   \n",
       "at achieving the modest , crowd-pleasing goals ...  ...      0     0    0   \n",
       "a patient viewer                                    ...      0     0    0   \n",
       "this new jangle of noise , mayhem and stupidity...  ...      0     0    0   \n",
       "\n",
       "                                                    zombie  zone  zoning  \\\n",
       "hide new secretions from the parental units              0     0       0   \n",
       "contains no wit , only labored gags                      0     0       0   \n",
       "that loves its characters and communicates some...       0     0       0   \n",
       "remains utterly satisfied to remain the same th...       0     0       0   \n",
       "on the worst revenge-of-the-nerds clichés the f...       0     0       0   \n",
       "...                                                    ...   ...     ...   \n",
       "a delightful comedy                                      0     0       0   \n",
       "anguish , anger and frustration                          0     0       0   \n",
       "at achieving the modest , crowd-pleasing goals ...       0     0       0   \n",
       "a patient viewer                                         0     0       0   \n",
       "this new jangle of noise , mayhem and stupidity...       0     0       0   \n",
       "\n",
       "                                                    zoom  zwick  zzzzzzzzz  \\\n",
       "hide new secretions from the parental units            0      0          0   \n",
       "contains no wit , only labored gags                    0      0          0   \n",
       "that loves its characters and communicates some...     0      0          0   \n",
       "remains utterly satisfied to remain the same th...     0      0          0   \n",
       "on the worst revenge-of-the-nerds clichés the f...     0      0          0   \n",
       "...                                                  ...    ...        ...   \n",
       "a delightful comedy                                    0      0          0   \n",
       "anguish , anger and frustration                        0      0          0   \n",
       "at achieving the modest , crowd-pleasing goals ...     0      0          0   \n",
       "a patient viewer                                       0      0          0   \n",
       "this new jangle of noise , mayhem and stupidity...     0      0          0   \n",
       "\n",
       "                                                    élan  \n",
       "hide new secretions from the parental units            0  \n",
       "contains no wit , only labored gags                    0  \n",
       "that loves its characters and communicates some...     0  \n",
       "remains utterly satisfied to remain the same th...     0  \n",
       "on the worst revenge-of-the-nerds clichés the f...     0  \n",
       "...                                                  ...  \n",
       "a delightful comedy                                    0  \n",
       "anguish , anger and frustration                        0  \n",
       "at achieving the modest , crowd-pleasing goals ...     0  \n",
       "a patient viewer                                       0  \n",
       "this new jangle of noise , mayhem and stupidity...     0  \n",
       "\n",
       "[67349 rows x 13774 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvectorizer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6f5f696-a535-42ec-9c4c-30a3eb1d42a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>zoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13770</th>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13771</th>\n",
       "      <td>zwick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13772</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13773</th>\n",
       "      <td>élan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13774 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word\n",
       "0            000\n",
       "1             10\n",
       "2            100\n",
       "3            101\n",
       "4            103\n",
       "...          ...\n",
       "13769     zoning\n",
       "13770       zoom\n",
       "13771      zwick\n",
       "13772  zzzzzzzzz\n",
       "13773       élan\n",
       "\n",
       "[13774 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa54790e-b5c6-4c35-928d-a33ad534609f",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code demonstrates how to transform text data by using CountVectorizer representation. To do that, we first import the necessary libraries, including pandas for data manipulation and CountVectorizer from sklearn.feature_extraction.text for converting text data into numerical vectors. Then, we ask the code to extract the text data from our sentence column in our training dataframe and convert it into a list using the tolist() method. Once done, we create an instance of the CountVectorizer class that will be used to transform the text data into a CountVectorizer representation. We fit the vectorizer on the text data using the fit_transform() method, so the code learns the vocabulary from the text data and transforms it into a sparse matrix representation. Then, we ask the code to convert the sparse matrix representation into a dataframe for visualization purposes. The resulting DataFrame countvectorizer_df contains the Countvectorizer representation of the text data, where each row corresponds to a movie review and each column corresponds to a unique word in the vocabulary. We then create a separate dataframe bow_df to store the vocabulary learned by the vectorizer, which will contain a list of all the unique words present in the text data. Finally, we print the first few rows of both the countvectorizer dataframe (countvectorizer_df) and the vocabulary dataframe (bow_df) to inspect the transformed data and the learned vocabulary.\n",
    "\n",
    "Note 2: In our current implementation, the sentence data retains digits and punctuation marks. While for sentiment analysis tasks, digits and punctuation marks are often considered irrelevant and are typically removed to enhance accuracy, in our case - as mentioned in Note 1 - our data undergoes tokenization during preprocessing. Therefore, the presence of digits and punctuation marks is not significant and would not impact our sentiment analysis results.\n",
    "On another hand, our decision to retain stop words comes from their significant role in preserving the integrity and context of the text. Even though they tend to appear frequently across documents yet add minimal semantic value, they often play crucial roles in defining the relationships between other words in a sentence. It is true that removing stopwords could enhance the quality of our analysis as it reduces noise in the dataset and focuses on more meaningful and informative words. However, the sentence structure might be altered, potentially leading to a loss of important information about the relationships between words. Indeed, in sentiment analysis, retaining stop words might lead to improved performance, as they can still contribute to the overall understanding of a document when considered collectively. Finally, removing stop words adds an additional preprocessing step, which can increase complexity and computational overhead. By retaining stop words, we can simplify the preprocessing step and potentially reduce the risk of inadvertently removing important information from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0eeae0-4c2e-4ba0-bb22-23d434d52254",
   "metadata": {},
   "source": [
    "Now we want to use the TF-IDF method for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863f67d7-ffdb-4a49-a84f-ed81ab0cbd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF DataFrame:\n",
      "                                                    000   10  100  101  103  \\\n",
      "hide new secretions from the parental units         0.0  0.0  0.0  0.0  0.0   \n",
      "contains no wit , only labored gags                 0.0  0.0  0.0  0.0  0.0   \n",
      "that loves its characters and communicates some...  0.0  0.0  0.0  0.0  0.0   \n",
      "remains utterly satisfied to remain the same th...  0.0  0.0  0.0  0.0  0.0   \n",
      "on the worst revenge-of-the-nerds clichés the f...  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                                    105  10th   11  110  112  \\\n",
      "hide new secretions from the parental units         0.0   0.0  0.0  0.0  0.0   \n",
      "contains no wit , only labored gags                 0.0   0.0  0.0  0.0  0.0   \n",
      "that loves its characters and communicates some...  0.0   0.0  0.0  0.0  0.0   \n",
      "remains utterly satisfied to remain the same th...  0.0   0.0  0.0  0.0  0.0   \n",
      "on the worst revenge-of-the-nerds clichés the f...  0.0   0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                                    ...  zishe  ziyi  zoe  \\\n",
      "hide new secretions from the parental units         ...    0.0   0.0  0.0   \n",
      "contains no wit , only labored gags                 ...    0.0   0.0  0.0   \n",
      "that loves its characters and communicates some...  ...    0.0   0.0  0.0   \n",
      "remains utterly satisfied to remain the same th...  ...    0.0   0.0  0.0   \n",
      "on the worst revenge-of-the-nerds clichés the f...  ...    0.0   0.0  0.0   \n",
      "\n",
      "                                                    zombie  zone  zoning  \\\n",
      "hide new secretions from the parental units            0.0   0.0     0.0   \n",
      "contains no wit , only labored gags                    0.0   0.0     0.0   \n",
      "that loves its characters and communicates some...     0.0   0.0     0.0   \n",
      "remains utterly satisfied to remain the same th...     0.0   0.0     0.0   \n",
      "on the worst revenge-of-the-nerds clichés the f...     0.0   0.0     0.0   \n",
      "\n",
      "                                                    zoom  zwick  zzzzzzzzz  \\\n",
      "hide new secretions from the parental units          0.0    0.0        0.0   \n",
      "contains no wit , only labored gags                  0.0    0.0        0.0   \n",
      "that loves its characters and communicates some...   0.0    0.0        0.0   \n",
      "remains utterly satisfied to remain the same th...   0.0    0.0        0.0   \n",
      "on the worst revenge-of-the-nerds clichés the f...   0.0    0.0        0.0   \n",
      "\n",
      "                                                    élan  \n",
      "hide new secretions from the parental units          0.0  \n",
      "contains no wit , only labored gags                  0.0  \n",
      "that loves its characters and communicates some...   0.0  \n",
      "remains utterly satisfied to remain the same th...   0.0  \n",
      "on the worst revenge-of-the-nerds clichés the f...   0.0  \n",
      "\n",
      "[5 rows x 13774 columns]\n",
      "\n",
      "Vocabulary DataFrame:\n",
      "            word\n",
      "0            000\n",
      "1             10\n",
      "2            100\n",
      "3            101\n",
      "4            103\n",
      "...          ...\n",
      "13769     zoning\n",
      "13770       zoom\n",
      "13771      zwick\n",
      "13772  zzzzzzzzz\n",
      "13773       élan\n",
      "\n",
      "[13774 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#We assume 'sentence' column contains the text of movie reviews in train_df\n",
    "sentence_data = train_df['sentence'].tolist()\n",
    "\n",
    "#We create a TfidfVectorizer instance with English stop words\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "\n",
    "#We fit the vectorizer on the text data and transform the text data into TF-IDF representation\n",
    "tfidf_train = vectorizer_tfidf.fit_transform(sentence_data)\n",
    "\n",
    "#We get the vocabulary with their corresponding indices\n",
    "vocabulary = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "#Then, we create a dataframe for the vocabulary\n",
    "vocabulary_df = pd.DataFrame({'word': vocabulary})\n",
    "\n",
    "#And convert TF-IDF representation to a dataframe for visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_train.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "\n",
    "#We set the index of the dataframe to our \"movie reviews\" (sentences)\n",
    "tfidf_df.index = sentence_data\n",
    "\n",
    "#Finally, we display the TF-IDF representation\n",
    "print(\"TF-IDF DataFrame:\")\n",
    "print(tfidf_df.head())\n",
    "\n",
    "#As well as the vocabulary\n",
    "print(\"\\nVocabulary DataFrame:\")\n",
    "print(vocabulary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776931d7-e3cb-4365-90c5-2d311a4fa1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>103</th>\n",
       "      <th>105</th>\n",
       "      <th>10th</th>\n",
       "      <th>11</th>\n",
       "      <th>110</th>\n",
       "      <th>112</th>\n",
       "      <th>...</th>\n",
       "      <th>zishe</th>\n",
       "      <th>ziyi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "      <th>élan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hide new secretions from the parental units</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contains no wit , only labored gags</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that loves its characters and communicates something rather beautiful about human nature</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remains utterly satisfied to remain the same throughout</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on the worst revenge-of-the-nerds clichés the filmmakers could dredge up</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a delightful comedy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anguish , anger and frustration</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at achieving the modest , crowd-pleasing goals it sets for itself</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a patient viewer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this new jangle of noise , mayhem and stupidity must be a serious contender for the title .</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 13774 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    000   10  100  101  103  \\\n",
       "hide new secretions from the parental units         0.0  0.0  0.0  0.0  0.0   \n",
       "contains no wit , only labored gags                 0.0  0.0  0.0  0.0  0.0   \n",
       "that loves its characters and communicates some...  0.0  0.0  0.0  0.0  0.0   \n",
       "remains utterly satisfied to remain the same th...  0.0  0.0  0.0  0.0  0.0   \n",
       "on the worst revenge-of-the-nerds clichés the f...  0.0  0.0  0.0  0.0  0.0   \n",
       "...                                                 ...  ...  ...  ...  ...   \n",
       "a delightful comedy                                 0.0  0.0  0.0  0.0  0.0   \n",
       "anguish , anger and frustration                     0.0  0.0  0.0  0.0  0.0   \n",
       "at achieving the modest , crowd-pleasing goals ...  0.0  0.0  0.0  0.0  0.0   \n",
       "a patient viewer                                    0.0  0.0  0.0  0.0  0.0   \n",
       "this new jangle of noise , mayhem and stupidity...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                                    105  10th   11  110  112  \\\n",
       "hide new secretions from the parental units         0.0   0.0  0.0  0.0  0.0   \n",
       "contains no wit , only labored gags                 0.0   0.0  0.0  0.0  0.0   \n",
       "that loves its characters and communicates some...  0.0   0.0  0.0  0.0  0.0   \n",
       "remains utterly satisfied to remain the same th...  0.0   0.0  0.0  0.0  0.0   \n",
       "on the worst revenge-of-the-nerds clichés the f...  0.0   0.0  0.0  0.0  0.0   \n",
       "...                                                 ...   ...  ...  ...  ...   \n",
       "a delightful comedy                                 0.0   0.0  0.0  0.0  0.0   \n",
       "anguish , anger and frustration                     0.0   0.0  0.0  0.0  0.0   \n",
       "at achieving the modest , crowd-pleasing goals ...  0.0   0.0  0.0  0.0  0.0   \n",
       "a patient viewer                                    0.0   0.0  0.0  0.0  0.0   \n",
       "this new jangle of noise , mayhem and stupidity...  0.0   0.0  0.0  0.0  0.0   \n",
       "\n",
       "                                                    ...  zishe  ziyi  zoe  \\\n",
       "hide new secretions from the parental units         ...    0.0   0.0  0.0   \n",
       "contains no wit , only labored gags                 ...    0.0   0.0  0.0   \n",
       "that loves its characters and communicates some...  ...    0.0   0.0  0.0   \n",
       "remains utterly satisfied to remain the same th...  ...    0.0   0.0  0.0   \n",
       "on the worst revenge-of-the-nerds clichés the f...  ...    0.0   0.0  0.0   \n",
       "...                                                 ...    ...   ...  ...   \n",
       "a delightful comedy                                 ...    0.0   0.0  0.0   \n",
       "anguish , anger and frustration                     ...    0.0   0.0  0.0   \n",
       "at achieving the modest , crowd-pleasing goals ...  ...    0.0   0.0  0.0   \n",
       "a patient viewer                                    ...    0.0   0.0  0.0   \n",
       "this new jangle of noise , mayhem and stupidity...  ...    0.0   0.0  0.0   \n",
       "\n",
       "                                                    zombie  zone  zoning  \\\n",
       "hide new secretions from the parental units            0.0   0.0     0.0   \n",
       "contains no wit , only labored gags                    0.0   0.0     0.0   \n",
       "that loves its characters and communicates some...     0.0   0.0     0.0   \n",
       "remains utterly satisfied to remain the same th...     0.0   0.0     0.0   \n",
       "on the worst revenge-of-the-nerds clichés the f...     0.0   0.0     0.0   \n",
       "...                                                    ...   ...     ...   \n",
       "a delightful comedy                                    0.0   0.0     0.0   \n",
       "anguish , anger and frustration                        0.0   0.0     0.0   \n",
       "at achieving the modest , crowd-pleasing goals ...     0.0   0.0     0.0   \n",
       "a patient viewer                                       0.0   0.0     0.0   \n",
       "this new jangle of noise , mayhem and stupidity...     0.0   0.0     0.0   \n",
       "\n",
       "                                                    zoom  zwick  zzzzzzzzz  \\\n",
       "hide new secretions from the parental units          0.0    0.0        0.0   \n",
       "contains no wit , only labored gags                  0.0    0.0        0.0   \n",
       "that loves its characters and communicates some...   0.0    0.0        0.0   \n",
       "remains utterly satisfied to remain the same th...   0.0    0.0        0.0   \n",
       "on the worst revenge-of-the-nerds clichés the f...   0.0    0.0        0.0   \n",
       "...                                                  ...    ...        ...   \n",
       "a delightful comedy                                  0.0    0.0        0.0   \n",
       "anguish , anger and frustration                      0.0    0.0        0.0   \n",
       "at achieving the modest , crowd-pleasing goals ...   0.0    0.0        0.0   \n",
       "a patient viewer                                     0.0    0.0        0.0   \n",
       "this new jangle of noise , mayhem and stupidity...   0.0    0.0        0.0   \n",
       "\n",
       "                                                    élan  \n",
       "hide new secretions from the parental units          0.0  \n",
       "contains no wit , only labored gags                  0.0  \n",
       "that loves its characters and communicates some...   0.0  \n",
       "remains utterly satisfied to remain the same th...   0.0  \n",
       "on the worst revenge-of-the-nerds clichés the f...   0.0  \n",
       "...                                                  ...  \n",
       "a delightful comedy                                  0.0  \n",
       "anguish , anger and frustration                      0.0  \n",
       "at achieving the modest , crowd-pleasing goals ...   0.0  \n",
       "a patient viewer                                     0.0  \n",
       "this new jangle of noise , mayhem and stupidity...   0.0  \n",
       "\n",
       "[67349 rows x 13774 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f7987ae-6606-4377-9fc0-add3680b9d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>zoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13770</th>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13771</th>\n",
       "      <td>zwick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13772</th>\n",
       "      <td>zzzzzzzzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13773</th>\n",
       "      <td>élan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13774 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word\n",
       "0            000\n",
       "1             10\n",
       "2            100\n",
       "3            101\n",
       "4            103\n",
       "...          ...\n",
       "13769     zoning\n",
       "13770       zoom\n",
       "13771      zwick\n",
       "13772  zzzzzzzzz\n",
       "13773       élan\n",
       "\n",
       "[13774 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f573e-7734-4ef2-8cb4-6dfdab288a27",
   "metadata": {},
   "source": [
    "##### Code explanation:\n",
    "\n",
    "The above code demonstrates how to create a TF-IDF (Term Frequency-Inverse Document Frequency) representation for our dataset. To do that, we firstly import the necessary libraries, namely pandas for data manipulation and sklearn's TfidfVectorizer for creating the TF-IDF representation. Then, we assume that the 'sentence' column in the dataframe 'train_df' contains the text of movie reviews, and ask the code to extract this data into a list called 'sentence_data'. Once done, we create the TfidfVectorizer instance with the parameter 'stop_words' set to 'english', which removes common English stop words during vectorization. The vectorizer is then fitted on the text data ('sentence_data') using the 'fit_transform' method, which allows the code to learn the vocabulary of the corpus and transforms the text data into TF-IDF representation. The vocabulary is then obtained using the 'get_feature_names_out' method, which will allow us to create a vocabulary dataframe 'vocabulary_df' to store the vocabulary, where each row corresponds to a unique word in the corpus. Then, the TF-IDF representation obtained from the vectorizer is converted into a dataframe called 'tfidf_df' for visualization purposes. Each row represents a review, and each column represents a unique word, with the cell values indicating the TF-IDF score for each word in each document. Finally, we display both the TF-IDF dataframe and the vocabulary dataframe to visualize the TF-IDF representation and the learned vocabulary, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc7bf8-1beb-4c7c-819b-ac6470c32443",
   "metadata": {},
   "source": [
    "#### b) Choose the most appropriate method based on its effectiveness in capturing the sentiment information. Consider providing explanations or justifications for your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3ac64-7cba-4722-a9ef-ff38a3cb8286",
   "metadata": {},
   "source": [
    "##### Recommendation:\n",
    "\n",
    "Considering the efficiency and effectiveness of both methods, as well as our training dataset label distribution, TF-IDF vectorization appears to be more suitable for our assignment. However, having a 55.78% for positive reviews and 44.22% for negative reviews, the choice between CountVectorizer and TF-IDF vectorization requires careful consideration. \n",
    "\n",
    "Indeed, as previously discussed, CountVectorizer method, while simple and quick to implement, treats each word independently and does not consider the importance of terms relative to their frequency across the entire corpus. This can lead to less accurate representations, especially in cases of stopwords. Additionally, it does not take into account the rarity of terms across documents, potentially resulting in less discriminative features.\n",
    "\n",
    "On the other hand, TF-IDF vectorization addresses these limitations by combining the term frequency (TF) and inverse document frequency (IDF) measures. TF-IDF assigns higher weights to terms that are frequent in a document but rare in the corpus, capturing their importance in representing the document. This results in more informative and more discriminative representations, as terms that are common across all documents are downweighted.\n",
    "\n",
    "Given that our task involves sentiment analysis on a large dataset of movie reviews, the enhanced representation provided by TF-IDF is crucial. Sentiment analysis requires capturing nuanced relationships between words and their importance in conveying sentiment. TF-IDF, with its ability to consider both term frequency and rarity across documents, offers a more accurate representation of the text data. Additionally, TF-IDF is better suited for handling larger datasets, as it provides improved discrimination between terms and reduces the risk of overfitting.\n",
    "\n",
    "Therefore, for our code, which likely involves sentiment analysis on a large dataset of movie reviews, with a varying distribution of positive and negative labels, TF-IDF vectorization would be a great choice for text representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d28c4b-730b-42fc-8620-ef55bf927b35",
   "metadata": {},
   "source": [
    "Before making any decision, we now want to compare both efficiency and effectiveness of our methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba257d5-83b1-46df-b621-7f5c70ec2a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using CountVectorizer: 0.8929472902746844\n",
      "Time taken using CountVectorizer: 2.5991508960723877 seconds\n",
      "Accuracy using TF-IDFVectorizer: 0.8853749072011878\n",
      "Time taken using TF-IDFVectorizer: 1.930722951889038 seconds\n"
     ]
    }
   ],
   "source": [
    "# Separate the features (X) and labels (y)\n",
    "X = train_df['sentence'].tolist()\n",
    "y = train_df['label'].tolist()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate using CountVectorizer\n",
    "start_time_cv = time.time()\n",
    "vectorizer_cv = CountVectorizer(lowercase=True)\n",
    "X_train_cv = vectorizer_cv.fit_transform(X_train)\n",
    "X_test_cv = vectorizer_cv.transform(X_test)\n",
    "clf_cv = LogisticRegression(max_iter=1000)\n",
    "clf_cv.fit(X_train_cv, y_train)\n",
    "y_pred_cv = clf_cv.predict(X_test_cv)\n",
    "accuracy_cv = accuracy_score(y_test, y_pred_cv)\n",
    "end_time_cv = time.time()\n",
    "time_taken_cv = end_time_cv - start_time_cv\n",
    "\n",
    "# Train and evaluate using TF-IDFVectorizer\n",
    "start_time_tfidf = time.time()\n",
    "vectorizer_tfidf = TfidfVectorizer(lowercase=True)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "end_time_tfidf = time.time()\n",
    "time_taken_tfidf = end_time_tfidf - start_time_tfidf\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy using CountVectorizer:\", accuracy_cv)\n",
    "print(\"Time taken using CountVectorizer:\", time_taken_cv, \"seconds\")\n",
    "print(\"Accuracy using TF-IDFVectorizer:\", accuracy_tfidf)\n",
    "print(\"Time taken using TF-IDFVectorizer:\", time_taken_tfidf, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd206f3-1624-47be-b58e-c398ccb1960f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation:\n",
    "\n",
    "The code above performs sentiment analysis using logistic regression classifiers trained on two different text representations: CountVectorizer and Term Frequency-Inverse Document Frequency (TF-IDF). \n",
    "\n",
    "To do that, we first import necessary libraries including pandas for data manipulation, train_test_split for splitting the dataset, logistic regression for classification, accuracy_score for evaluating model performance, and CountVectorizer and TfidfVectorizer for text representation. Then, we load the dataset train_df containing 'sentence' and 'label' columns, and split it into training and testing sets using train_test_split. After that, we use our predrefined vectorizer_cv to transform the training and testing text data into CountVectorizer representation (X_train_cv and X_test_cv) and finally train a logistic regression classifier (clf_cv) on our representation. After that, we do the same for TF-IDF representation, where we transform the training and testing text data into TF-IDF representation (X_train_tfidf and X_test_tfidf) and train a logistic regression classifier (clf_tfidf) on it.\n",
    "\n",
    "Once done, we evaluate the model performance by predicting the labels of the testing set using both classifiers (clf_cv and clf_tfidf) and calculate the accuracy scores for both models using accuracy_score. Then, we measure the computational time taken to convert the validation set (X_val) into CountVectorizer and TF-IDF representations. Finally, we print the accuracy scores and computational time taken for converting the validation set into CounterVectorizer and TF-IDF representations of both models. \n",
    "\n",
    "Note 3: In our above code, X_train and y_train are used for training the model, while X_test and y_test are used for evaluating the trained model's performance. The variables X_train and X_test hold the features for the training and testing sets, respectively. Each row in X_train or X_test represents a sample (movie review), and each column represents a different aspect of that sample (words). We use X_train to teach the model and X_test to see how well it learned. Regarding y_train and y_test variables, these hold the corresponding labels for the training and testing sets, respectively. Each value in y_train or y_test represents the true sentiment of the corresponding sample in X_train or X_test. We use y_train to teach the model what each sample's category is, and y_test to check if the model predicted the categories correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76e493-8dd9-4918-b260-04ff062b4a3f",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "Based on the provided results, we notice that both ContVectorizer and TF-IDF representations took almost the same amount of time to compute, with CountVectorizer (2.36 seconds) being slightly slower than TF-IDF(1.73 seconds). ##Therefore, the difference in computational time is negligible. ## However, in terms of accuracy, the logistic regression model trained on the CountVectorizer representation is slightly higher (0.89) compared to the TF-IDF representation (0.88). This indicates that the model performed better on the validation set when using the CountVectorizer representation. Therefore, based on these results, if we prioritize accuracy, the CountVectorizer representation appears to be more accurate and better to use for this specific dataset and task. \n",
    "\n",
    "\n",
    "Note 3: Prior to developping the above code, we generated the accuracy for both CountVectorizer and TF-IDF while using our training dataset removing stopwords. We noticed that when stop words are removed, the accuracy of both CountVectorizer(0.88) and TF-IDF(0.87) is lower compared to when stop words are retained (0.89 and 0.88 respectively). This suggests that stop words might contain some valuable information for sentiment analysis, and removing them reduces the model's performance. Moreover, when stop words are removed, the computational time for both CountVectorizer (1.35 seconds) and TF-IDF(1.40 seconds) representations is lower compared to when stop words are retained (2.36 seconds and 1.73 seconds respectively). This is likely due to the larger vocabulary size when retaining stop words. Please note that our results align with our Note 2. \n",
    "However, we notice that whether we remove or retain stopwords, the accuracy and computational time of both representations are very close and the CountVectorizer representation is always the most accurate. Therefore, we will be using the ContVectorizer representation to run our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b3fde-dd51-43fb-88f2-066e36487d0a",
   "metadata": {},
   "source": [
    "## III - The machine learning model of choice (15 marks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1d018-82e9-40dc-bd34-ab71055f3fcc",
   "metadata": {},
   "source": [
    "#### a) Compare and contrast the efficiency of at least two models for classification we have studied in class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68ff79-89eb-44b4-bf27-28bc4a5c9f27",
   "metadata": {},
   "source": [
    "Based on the analysis above, we have determined that when comparing metrics such as accuracy, precision, recall, and F1-score, it is essential to divide our training dataset into separate train and test subsets. For subsequent questions, please refer to the following data split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13994f8-c561-447f-afec-36b96c9464a3",
   "metadata": {},
   "source": [
    "In the below code, we aim to split our data set into random train and test subsets. By default,  80% of our training dataset is reserved for training and the 20% for testing. This aligns with our previous code that aimed at comparing both efficiency and effectiveness of our CountVectorizer and TF-IDF representation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a37847-e846-40ce-8752-722904c3cebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 53879\n",
      "Test set size: 13470\n"
     ]
    }
   ],
   "source": [
    "#We assume train_df contains our training dataset\n",
    "\n",
    "#We split the dataset into features (X) and labels (y)\n",
    "X = train_df['sentence']  #We assuming 'sentence' is the column containing the text data\n",
    "y = train_df['label']     #And 'label' is the column containing the corresponding labels\n",
    "\n",
    "#Then, we split the dataset into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#We create new dataframes for the train and test subsets\n",
    "train80_df = pd.DataFrame({'sentence': X_train, 'label': y_train})\n",
    "test20_df = pd.DataFrame({'sentence': X_test, 'label': y_test})\n",
    "\n",
    "#Finally, we print the sizes of the train and test subsets\n",
    "print(\"Training set size:\", train80_df.shape[0])\n",
    "print(\"Test set size:\", test20_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232993e-c45a-4cb5-913e-cff796e77512",
   "metadata": {},
   "source": [
    "##### Code explanation:\n",
    "\n",
    "The above code aims to split our training dataset into random train and test subsets. To do that, we first ask the code to import the required libraries such as pandas for data manipulation and train_test_split from sklearn.model_selection for splitting the dataset into train and test subsets. Assuming that train_df contains our original training dataset, it extracts the features (X) and labels (y) from it and assumes that the 'sentence' column contains the text data and the 'label' column contains the corresponding labels. Then, \n",
    "the code splits the dataset into train and test subsets using the train_test_split function. The test_size=0.2 argument specifies that 20% of the data will be reserved for testing, while the remaining 80% will be used for training. The random_state=42 argument ensures reproducibility by fixing the random seed, so the same split is obtained each time the code is run. Once done, we ask the code to create two new dataframe, train80_df and test20_df, to store the train and test subsets respectively. These dataframes consist of two columns: 'sentence' and 'label', representing the text data and corresponding labels. Finally, we ask the code to print the sizes of the train and test subsets for verification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7811c0-4343-4c3a-b44d-0ca0c0d82301",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "The aim of this code is to split our training dataset into random train and test subsets. As per our previous code, we set a 80% of the training dataset to be reserved for training, while the remaining 20% to be allocated for testing. This splitting process is essential for evaluating the performance of machine learning models. The train subset is used to train the model, while the test subset is used to evaluate its performance and generalization ability on unseen data. This ensures that the model does not overfit to the training data and performs well on new, unseen data.\n",
    "\n",
    "From the above code output, we notice that our Training set size is 53879 while our Test set size is 13470. Knowing that our original training dataset had a size of 67349, we can confirm that our original training dataset has been split correctly, and that the train and test subsets have the expected sizes meaning that we can proceed with further analysis or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7182c1b7-25e0-4486-91da-d9457ea0ac65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65374</th>\n",
       "      <td>lacks the inspiration of the original and has ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14599</th>\n",
       "      <td>is also a film of freshness , imagination and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46387</th>\n",
       "      <td>far more alienating than involving</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27872</th>\n",
       "      <td>the creative animation work may not look as fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40668</th>\n",
       "      <td>make for a winning , heartwarming yarn .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>its provocative conclusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>an action film disguised as a war tribute is d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>goes to absurd lengths to duck the very issues...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>a perfect performance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>constantly pulling the rug from underneath us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53879 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "65374  lacks the inspiration of the original and has ...      0\n",
       "14599  is also a film of freshness , imagination and ...      1\n",
       "46387                far more alienating than involving       0\n",
       "27872  the creative animation work may not look as fu...      0\n",
       "40668          make for a winning , heartwarming yarn .       1\n",
       "...                                                  ...    ...\n",
       "37194                        its provocative conclusion       1\n",
       "6265   an action film disguised as a war tribute is d...      0\n",
       "54886  goes to absurd lengths to duck the very issues...      0\n",
       "860                               a perfect performance       1\n",
       "15795     constantly pulling the rug from underneath us       1\n",
       "\n",
       "[53879 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train80_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcafefc1-ba0c-40e8-82ad-2258eaf7982b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66730</th>\n",
       "      <td>with outtakes in which most of the characters ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29890</th>\n",
       "      <td>enigma is well-made</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45801</th>\n",
       "      <td>is ) so stoked to make an important film about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29352</th>\n",
       "      <td>the closest thing to the experience of space t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19858</th>\n",
       "      <td>lose their luster</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34702</th>\n",
       "      <td>makes a nice album</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23754</th>\n",
       "      <td>sent back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58408</th>\n",
       "      <td>do n't think so</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5637</th>\n",
       "      <td>works beautifully as a movie without sacrifici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26351</th>\n",
       "      <td>love with its overinflated mythology that it n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "66730  with outtakes in which most of the characters ...      0\n",
       "29890                               enigma is well-made       1\n",
       "45801  is ) so stoked to make an important film about...      0\n",
       "29352  the closest thing to the experience of space t...      1\n",
       "19858                                 lose their luster       0\n",
       "...                                                  ...    ...\n",
       "34702                                makes a nice album       1\n",
       "23754                                         sent back       0\n",
       "58408                                   do n't think so       0\n",
       "5637   works beautifully as a movie without sacrifici...      1\n",
       "26351  love with its overinflated mythology that it n...      0\n",
       "\n",
       "[13470 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test20_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cef469-fc5f-4e7a-b19d-8afa0d45c1dd",
   "metadata": {},
   "source": [
    "For this question, we will be comparing and contrasting the efficiency of Naive Bayes and Logistic Regression model classification for movie reviews in the dataset.\n",
    "\n",
    "First, we will examine each method individually before making a final comparison and recommendation.\n",
    "\n",
    "##### Naive Bayes:\n",
    "\n",
    "Naive Bayes is a simple probabilistic classifier based on Bayes' theorem with the assumption of independence between features. It is commonly used for classification tasks, especially in NLP and text classification (Ray, 2024). It consists of multiple steps such as data preprocessing which aims to clean and preprocess our dataset, including tokenization, lowercasing, and removing stop words. Then, it converts the text data into numerical feature vectors using techniques such as CountVectorizer or TF-IDF (Grad blog, no date). Moreover, it estimates the conditional probabilities of each feature given the class label using the training data, calculates the posterior probability of each class given the features using Bayes' theorem and selects the class with the highest probability. Once done, it finally evaluate the performance of the model using metrics such as accuracy, precision, recall, and F1-scoreup (Turing, 2022).\n",
    "\n",
    "This method is straightforward to implement and understand, computationally efficient and can handle large datasets with high-dimensional feature spaces, as it has an efficient memory. Therefore, it can perform well even when some features are irrelevant or redundant, as long as they are conditionally independent given the class label. However, given that the model assumes that features are conditionally independent, it may not hold true in real-world datasets which can lead to suboptimal performance if features are highly correlated. In cases like this, it may provide poor estimates for rare events or classes with very few instances. This means that the model cannot capture interactions between features, which limits its ability to model complex relationships in the data and may not perform well with features that have complex distributions (Bele, 2023).\n",
    "\n",
    "\n",
    "##### Logistic Regression:\n",
    "\n",
    "Logistic Regression is a statistical model used for binary classification tasks, where the output variable  is categorical and has two possible outcomes. It estimates the probability that a given input belongs to a particular category based on one or more predictor variables (javatpoint, no date). It consists of multiple steps such as cleaning and preprocessing the dataset, including handling missing values, encoding categorical variables, and scaling numerical features if necessary. Then, the next step consists of selecting relevant features or performing dimensionality reduction techniques to reduce the number of input variables. Subsequently, it estimates the parameters of the logistic regression model using training data through optimization algorithms such as gradient descent or maximum likelihood estimation. Finally, the performance of the trained model is assessed using evaluation metrics such as accuracy, precision, recall, F1-score, and the trained logistic regression model is used to make predictions on new data by estimating the probability of the outcome variable belonging to a particular class (Bhor, 2024).\n",
    "\n",
    "This method is easy to interpret and understand, making it suitable for explaining the relationship between predictors and the target variable. It also is computationally efficient and can handle large datasets with a relatively low computational cost and provides probabilistic outputs, allowing easy interpretation of model confidence levels. Moreover, this method can estimate the importance of features in predicting the outcome by examining the coefficients of the model and perform well even when there is noise in the data (Kaggle, no date).\n",
    "However, Logistic Regression assumes a linear relationship between the independent variables and the log odds of the dependent variable. It may not perform well if the relationship is non-linear. Indeed, it cannot capture complex relationships between features as it is a linear model and may underperform when dealing with highly non-linear data. It is also highly sensitive to outliers in the data as they can disproportionately influence the estimated coefficients, potentially leading to biased predictions. Finally, as it is inherently a binary classifier it may require extensions or modifications for multi-class classification tasks (GfG, 2023).\n",
    "\n",
    "\n",
    "##### Comparison:\n",
    "Naive Bayes and Logistic Regression are both popular classification methods with distinct characteristics. Naive Bayes is known for its simplicity and computational efficiency, assuming feature independence given the class label. This makes it fast to train and highly scalable, particularly suited for large datasets with high-dimensional feature spaces. On the other hand, Logistic Regression offers more flexibility by modeling the probability of a binary outcome based on predictor variables. While still relatively simple, it can capture non-linear relationships between features and the target variable. However, Logistic Regression may be slower to train compared to Naive Bayes, especially with large datasets, and it can be sensitive to multicollinearity and irrelevant features. Both models perform well on imbalanced data, but Naive Bayes is particularly adept when class separation is distinct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c4359-122f-45da-9c85-382edc2c488c",
   "metadata": {},
   "source": [
    "##### Recommendation:\n",
    "\n",
    "Based on the characteristics of our training dataset, both Naive Bayes and Logistic Regression can be viable options.\n",
    "\n",
    "Naive Bayes is known for its simplicity, fast training speed, and robust performance on imbalanced datasets, particularly when class separation is distinct. Given that the training dataset exhibit some class imbalance, with a more pronounced difference, Naive Bayes could effectively capture the class distributions and perform well in classification tasks.\n",
    "\n",
    "Logistic Regression, on the other hand, offers more flexibility and can capture complex relationships between features and the target variable. While it may require more computational resources and could be sensitive to multicollinearity, it could potentially provide better discrimination between classes and better handle subtle differences in the dataset.\n",
    "\n",
    "Ultimately, the choice between Naive Bayes and Logistic Regression will depend on the specific objectives of the classification task, the desired interpretability of the model, and the available computational resources. If simplicity and efficiency are prioritized, Naive Bayes may be the preferred choice. However, if flexibility and the ability to capture more complex relationships are necessary, Logistic Regression could be more suitable. Therefore, we will experiment with both models and evaluate their performance metrics such as accuracy, precision, recall, and F1-score to determine which one best meets the requirements of our task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b3003-9040-4c20-8b96-fc7cd339db17",
   "metadata": {},
   "source": [
    "#### b) Evaluate their performance in terms of accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90e2dd-40b2-4bcf-b75e-8ac33e06ee5f",
   "metadata": {},
   "source": [
    "We first evaluate the performance for Naive Bayes method while using both CountVectorizer and TfidfVectorizer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e6e42a8-e525-4fa7-aeda-2155d9b68aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13656\n",
      "Training Accuracy: 90.82%\n",
      "Test Accuracy: 87.97%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      5909\n",
      "           1       0.88      0.90      0.89      7561\n",
      "\n",
      "    accuracy                           0.88     13470\n",
      "   macro avg       0.88      0.88      0.88     13470\n",
      "weighted avg       0.88      0.88      0.88     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We assume train80_df and test20_df are already created\n",
    "\n",
    "#We extract features (X) and labels (y) from the training and testing datasets\n",
    "X_train = train80_df['sentence']\n",
    "y_train = train80_df['label']\n",
    "X_test = test20_df['sentence']\n",
    "y_test = test20_df['label']\n",
    "\n",
    "#We convert text data into numerical feature vectors using TfidfVectorizer\n",
    "vectorizer_nb = TfidfVectorizer().fit(X_train)\n",
    "X_train_counts = vectorizer_nb.transform(X_train)\n",
    "X_test_counts = vectorizer_nb.transform(X_test)  \n",
    "\n",
    "#We print the number of features\n",
    "print(\"Number of features:\", X_train_counts.shape[1])\n",
    "\n",
    "#We then train Naive Bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "nb_model = naive_bayes_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "#And make predictions on the training data\n",
    "y_train_pred = naive_bayes_classifier.predict(X_train_counts)\n",
    "\n",
    "#Then, we evaluate performance on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred) * 100\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracy))\n",
    "\n",
    "#Make predictions on the test data\n",
    "y_test_pred = naive_bayes_classifier.predict(X_test_counts)\n",
    "\n",
    "#To then evaluate performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy))\n",
    "\n",
    "#We finally print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf435339-e646-401a-a1db-10b2d615ca75",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code performs text classification using a Multinomial Naive Bayes classifier. To do that, we ask the code to import necessary libraries such as pandas for data manipulation, TfidfVectorizer for converting text data into numerical feature vectors, MultinomialNB for for training the Multinomial Naive Bayes classifier, and accuracy_score, classification_report for evaluation metrics. We then ask the code to extract features (sentences) and labels from both our training (train80_df) and testing (test20_df) datasets. Once done, we ask the code to extract the feature (X_train) and label (y_train) columns from our train80_df dataframe. Then TfidfVectorizer is used to convert the text data into numerical feature vectors for both the training and testing data. fit is used on the training data (X_train) to learn the vocabulary, and transform is used on both training and testing data to transform the text into feature matrices based on the learned vocabulary. Then, we ask the code to print the number of features (unique words) in the training set using the shape attribute of the transformed training data. After that, the code initializes a Multinomial Naive Bayes classifier (naive_bayes_classifier) and trains it using the feature vectors (X_train_counts) and its corresponding labels (y_train). \n",
    "\n",
    "This leads the code to use the trained classifier to make predictions on the testing data (X_test_counts) to evaluate its performance, which consists in calculating the accuracy of the classifier by comparing the predicted labels (y_train_pred) with the actual labels (y_train). In other words, the Naive Bayes classifier is trained on the training data (X_train_counts) while predictions are made on the testing data (X_test_counts) using the trained classifier. Finally, we generate a classification report based on the classifier performance evaluation on the test set (test20_df) using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bf0df-155f-42e0-927e-e51dbf773419",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "The above results provide insights into the performance of a Naive Bayes classifier trained on sentiment analysis data. We notice that the classifier contains 13,656 features, meaning that it has a rich vocabulary to learn from, capturing a wide range of words or tokens present in the text data. Moreover, the high training accuracy of 90.82% indicates that the classifier has learned well from the training data. It correctly predicted the sentiment of over 90% of the samples in the training set. In terms of test accuracy (87.97%) suggests that the classifier generalizes reasonably well to unseen data. It correctly predicted the sentiment of nearly 88% of the samples in the test set. Then, our classification report indicated a reasonably high values for precision and recall for both classes (0 and 1) (0.87 for class 0/ 0.88 for class 1; 0.85 for class 0/ 0.90 for class 1, respectively). Indeed, higher precision and recall indicates fewer false positive and fewer false negative indicating a good balance between them. \n",
    "Finally, the F1-score, which is the harmonic mean of precision and recall, for both classes is relatively high, indicating good overall performance. \n",
    "\n",
    "\n",
    "Note 4: Prior to developing the code presented above, we decided to utilize the CountVectorizer representation for our model. Upon evaluating the performance of Naive Bayes with CountVectorizer, we observed slightly lower overall performance compared to Naive Bayes with TfidfVectorizer. Specifically, the training accuracy was 89.69% and the test accuracy was 87.39% with CountVectorizer, while with TfidfVectorizer, the training accuracy was 90.82% and the test accuracy was 87.97%. However, upon examining the classification report, we found that the F1-scores for both classes 0 and 1 were similar between Naive Bayes with CountVectorizer and Naive Bayes with TfidfVectorizer (0.86 for class 0 and 0.89 for class 1). Nevertheless, the macro average and weighted average were slightly lower with CountVectorizer (0.87) compared to TfidfVectorizer (0.88). This suggests that while the two approaches perform similarly in terms of F1-score for individual classes, the TfidfVectorizer may offer a slight advantage in terms of overall performance metrics such as macro average and weighted average. However, the differences are relatively minor, and other factors such as computational efficiency and ease of implementation should also be considered when choosing between the two representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a4a35-1a04-4c68-bc2a-fc4c1ddbcab1",
   "metadata": {},
   "source": [
    "We then evaluate the performance for Logistic Regression method with both CountVectorizer and TfidfVectorizer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c24f072-bdbd-4d54-a69e-ee0c3ca85ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13656\n",
      "Training Accuracy: 93.98%\n",
      "Test Accuracy: 89.29%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      5909\n",
      "           1       0.90      0.91      0.91      7561\n",
      "\n",
      "    accuracy                           0.89     13470\n",
      "   macro avg       0.89      0.89      0.89     13470\n",
      "weighted avg       0.89      0.89      0.89     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We assume train80_df and test20_df are already created\n",
    "\n",
    "#We extract features (X) and labels (y) from the training and testing datasets\n",
    "X_train = train80_df['sentence']\n",
    "y_train = train80_df['label']\n",
    "X_test = test20_df['sentence']\n",
    "y_test = test20_df['label']\n",
    "\n",
    "#We convert text data into numerical feature vectors using CountVectorizer\n",
    "vectorizer_lr = CountVectorizer().fit(X_train)\n",
    "X_train_counts = vectorizer_lr.transform(X_train)\n",
    "X_test_counts = vectorizer_lr.transform(X_test)  \n",
    "\n",
    "#We print the number of features\n",
    "print(\"Number of features:\", X_train_counts.shape[1])\n",
    "\n",
    "#We then train Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_model = logistic_regression_classifier.fit(X_train_counts, y_train)\n",
    "\n",
    "#And make predictions on the training data\n",
    "y_train_pred = logistic_regression_classifier.predict(X_train_counts)\n",
    "\n",
    "#Then, we evaluate performance on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred) * 100\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracy))\n",
    "\n",
    "#Make predictions on the test data\n",
    "y_test_pred = logistic_regression_classifier.predict(X_test_counts)\n",
    "\n",
    "#To then evaluate performance on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred) * 100\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy))\n",
    "\n",
    "#We finally print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28132041-5221-4c89-a2b1-65c617a185a8",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "\n",
    "To not reiterate the code explanation, it is similar to the above code but instead of using Naive Bayes, we use Logistic Regression classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16bb2a-9bdc-4109-80ed-2d5ad6a59a3c",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "The above results provide insights into the performance of a Logistic Regression classifier trained on sentiment analysis data. We notice that the classifier contains 13,656 features, meaning that it has a rich vocabulary to learn from, capturing a wide range of words or tokens present in the text data. Moreover, the high training accuracy of 93.98% indicates that the classifier has learned well from the training data. It correctly predicted the sentiment of over 90% of the samples in the training set. In terms of test accuracy (89.29%) it suggests that the classifier generalizes reasonably well to unseen data. It correctly predicted the sentiment of nearly 89% of the samples in the test set. Then, our classification report indicates a reasonably high values for precision and recall for both classes (0 and 1) (0.89 for class 0/ 0.90 for class 1; 0.87 for class 0/ 0.91 for class 1, respectively). Indeed, higher precision and recall indicates fewer false positive and fewer false negative indicating a good balance between them. Finally, the F1-score, which is the harmonic mean of precision and recall, for both classes is relatively high, indicating good overall performance.\n",
    "\n",
    "Note 5: Prior to developing the code presented above, we decided to utilize the TfidfVectorizer representation for our model. Upon evaluating the performance of Logistic Regression with TfidfVectorizer, we observed slightly lower overall performance compared to Logistic Regression with CountVectorizer. Specifically, the training accuracy was 92.27% and the test accuracy was 88.54% with TfidfVectorizer, while with CountVectorizer, the training accuracy was 93.98% and the test accuracy was 89.29%. However, upon examining the classification report, we found that the F1-scores for both classes 0 and 1 were slightly higher with Logistic Regression with CountVectorizer (0.88 for class 0 and 0.91) than Logistic Regression with TfidfVectorizer (0.87 for class 0 and 0.90 for class 1). Moreover, the macro average and weighted average were slightly lower with TfIdfVectorizer (0.88 and 0.89 respectivevly) compared to CountVectorizer (0.89 and 0.89 respectively). \n",
    "This suggests that while TfidfVectorizer may offer a different perspective on the data, CountVectorizer might be more suitable for capturing certain nuances in the text data. However, the differences are relatively minor, and other factors such as computational efficiency and ease of implementation should also be considered when choosing between the two representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55088db8-4142-4475-ba8c-f5fdba9e5879",
   "metadata": {},
   "source": [
    "#### c) Discuss the strengths and weaknesses of each model and select the one that achieves the highest accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27343ef2-b305-4165-b0b1-fa6051ede3ae",
   "metadata": {},
   "source": [
    "Based on our performance analysis, both Naive Bayes model with TfidfVectorizer and Logistic Regression model with CountVectorizer exhibit a rich vocabulary, trained on a dataset with 13,656 features, indicating a wide range of captured words from the trained data. While both models achieve high training accuracy, with Naive Bayes reaching 90.82% and Logistic Regression achieving 93.98%, they demonstrate good learning from the training data.\n",
    "\n",
    "In terms of test accuracy, both models perform reasonably well, with Naive Bayes achieving 87.97% and Logistic Regression reaching 89.29%. This indicates that both models demonstrates reasonable generalization to unseen data. Similarly, both models display high precision and recall for both classes, suggesting a good balance between false positives and false negatives. The F1-scores for both classes are also relatively high, indicating strong overall performance.\n",
    "\n",
    "However, Logistic Regression with CountVectorizer shows slightly higher F1-scores for individual classes compared to Naive Bayes with TfidfVectorizer. This could be attributed to Logistic Regression's ability to capture complex relationships between features and the target variable, indicating superior performance. \n",
    "\n",
    "Given the higher accuracy and slightly better performance metrics across all measures, Logistic Regression with CountVectorizer emerges as the preferred choice for our classification task. It strikes a balance between model complexity and performance, making it well-suited for our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2491343-1fad-495b-b906-1bf845836fdf",
   "metadata": {},
   "source": [
    "## IV - The hyper-parameter values of choice (15 marks). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021fc9f-6002-435f-a15f-3b21ad42ebe0",
   "metadata": {},
   "source": [
    "#### a) Explore at least two hyper-parameters for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1a863-5c84-4cfd-b3a7-31ff6a55f913",
   "metadata": {},
   "source": [
    "For this question, we will be comparing and contrasting the efficiency of two hyper-parameters, Regularization Strength (C) and Class Weight, for Logistic Regression with CountVectorizer  model classification for movie reviews in our dataset.\n",
    "\n",
    "First, we will examine each method individually before making a final comparison and recommendation.\n",
    "\n",
    "##### C (Regularization Strength):\n",
    "\n",
    "Regularization Strength (C) serves as a pivotal hyperparameter in logistic regression, offering control over the amount of regularization applied to our model (scikit, 2024). By penalizing large coefficients, regularization effectively prevents overfitting, ensuring the model generalizes well to unseen data. In fact, the hyperparameter C represents the inverse of regularization strength, implying that lower values of C result in stronger regularization, curbing model complexity to prevent overfitting. Conversely, higher values of C weaken regularization, allowing the model to closely fit the training data (KDnuggets, no date).\n",
    "\n",
    "Effective tuning of the C hyperparameter involves several iterative steps. Initially, a range of values for C is selected, often through a wide-to-narrow search approach, with adjustments guided by performance results. Subsequently, a grid search is performed across the specified range to evaluate model performance for various C values. Utilizing cross-validation techniques, such as k-fold cross-validation, fvaluate the performance of the model for each hyperparameter setting, hence avoiding overfitting. Finally, the value of C yielding optimal performance on the validation set is identified and fine-tuned to achieve peak efficacy (Brownlee, 2020).\n",
    "\n",
    "By tuning C hyperparameter, we can control the regularization strength in logistic regression, to avoid overfitting and improve the model's generalization cability. This also allows us to adjust the trade-off between fitting the training data well and maintaining simplicity in the model to find the optimal balance for our specific dataset. Moreover, customization of regularization strength according to our dataset nuances fosters enhanced model adaptability and performance. However, despite tuning C offering substantial benefits, it comes with challenges. Indeed, performing a grid search over a range of C values can be computationally expensive, especially for large datasets or when using cross-validation. Moreover, there is an overfitting risk when performing an improper C tuning. Indeed, finding the optimal value of C is subjective and requires thoughtful consideration and validation to ensure that the model generalizes well to unseen data and avoid suboptimal choice (Avila, 2023). \n",
    "\n",
    "\n",
    "##### Class Weight:\n",
    "\n",
    "Class weight is a hyperparameter commonly employed in various machine learning algorithms, including logistic regression, to tackle imbalanced datasets effectively. This parameter allows us to assign different weights to each class in the training data, thereby enabling the model to prioritize learning from minority classes. By default, all classes are assigned equal weight (1.0), but adjusting these weights based on class frequencies helps address class imbalance issues (scikit, 2024). In logistic regression implementation within scikit-learn, the class_weight parameter can be set to 'balanced', automatically adjusting weights inversely proportional to class frequencies. This means assigning more weight to minority classes and less weight to majority classes, thereby enabling the model to focus on learning from underrepresented classes (scikit, 2024).\n",
    "\n",
    "The process involving this hyperparameter entails several steps. Initially, assessing the class distribution in the training dataset helps determine the extent of class imbalance. Subsequently, if significant imbalance exists, deciding on a weighting strategy involves assigning higher weights to minority classes. Once determined, class weights can be specified either manually or using the 'balanced' option. Finally, training the logistic regression model with the specified class weights allows for performance evaluation and comparison with unweighted models to assess the impact on overall performance (TensorFlow, no date).\n",
    "\n",
    "By implementing the class weight hyperparameter, which is relatively simple and straightforward, the issue of class imbalance is addressed by ensuring that the model allocates more attention to minority classes during training, thereby improving performance metrics such as accuracy, precision, recall, and F1-score, particularly on imbalanced datasets. However, the effectiveness of class weights depends on the degree of class imbalance. If the class distribution is highly skewed, assigning class weights may not be sufficient to address the imbalance effectively. Moreover, assigning high weights to minority classes may lead to overfitting, especially when the number of instances in these classes is very small as well as result in a loss of information from majority classes potentially leading to biased predictions. Therefore, rigorous validation and monitoring of model performance are essential to mitigate such risks (Canuma, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8f6c7-6b60-4ff3-89a5-6f484a2bdd49",
   "metadata": {},
   "source": [
    "#### b) Explain the rationale behind the selection of the explored hyperparameters by performing a systematic search and evaluation of different hyperparameter values to optimise the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8f1b8-cee9-4754-bafa-8473f8d2dc95",
   "metadata": {},
   "source": [
    "The rationale behind the selection of the explored hyperparameters, C (Regularization Parameter) and Class Weight, is influenced by the characteristics of our training dataset (training set size (train80_df) of 53879 while test set size (test20_df) is 13470) and the observed model performance.\n",
    "\n",
    "##### Class Weight:\n",
    "\n",
    "Given the class imbalance in our dataset, where positive reviews are more prevalent than negative reviews (55.78% positive reviews vs. 44.22% negative reviews), it is essential to address this issue to prevent our model from being biased towards the majority class. To address this class imbalance, the Class Weight hyperparameter is particularly relevant. By exploring different weighting strategies for the classes, such as assigning higher weights to minority classes (in this case, the negative reviews) or using the 'balanced' option, we can ensure that our model gives equal consideration to all classes during training.\n",
    "Since logistic regression achieved high accuracy on the training set (93.98%), the Class Weight hyperparameter can further improve model performance by ensuring better handling of imbalanced classes, potentially leading to improved accuracy, precision, recall, and F1-score, particularly for the minority class. Indeed, the systematic search and evaluation of class weights allow us to identify the weighting strategy that leads to the best performance. Therefore, by adjusting the class weights based on the class frequencies in the training data, we can improve the model's ability to learn from minority class instances and make more accurate predictions on imbalanced datasets.\n",
    "\n",
    "##### C (Regularization Parameter):\n",
    "\n",
    "With high training accuracy (93.98%) and test accuracy (89.29%) that suggest that our model performs well on our dataset, there is a risk of overfitting, especially given the complexity of the model. Therefore, to optimize its performance further, tuning the Regularization Parameter (C) allows for balancing between fitting the training data well and preventing overfitting. By selecting an appropriate value for C, we can control the regularization strength, ensuring that the model generalizes well to unseen data while maintaining an optimal balance between model complexity and performance. Furthermore, our logistic regression model with CountVectorizer exhibits a rich vocabulary, capturing a wide range of words from the text data. This richness in vocabulary contributes to the high dimensionality of the feature space, with 13,656 features. In models with such a large number of features, regularization becomes even more critical to prevent overfitting. Through systematic search and evaluation of different values of the regularization parameter C, we can control the model's complexity and address potential overfitting issues. By varying the value of C and evaluating the model's performance using appropriate metrics, such as accuracy, precision, recall, and F1-score, we can identify the optimal value of C that balances model complexity and generalization performance. Therefore, tuning the regularization parameter (C) is essential for maximizing the effectiveness of logistic regression models, particularly in the context of sentiment analysis. By systematically exploring and evaluating different values of C, we ensure that the model not only fits the training data well but also generalizes effectively to unseen data, thereby enhancing its real-world applicability and performance.\n",
    "\n",
    "\n",
    "\n",
    "##### Recommendation: \n",
    "\n",
    "Given that our primary goal is to achieve high accuracy, both the Class Weight and the Regularization Parameter (C) hyperparameters can play crucial roles in improving model performance.\n",
    "\n",
    "Given the class imbalance in our dataset, where positive reviews are more prevalent than negative reviews, it is essential to address this issue to prevent our model from being biased towards the majority class. Therefore, by using Class Weight hyperparameter to explore different weighting strategies for the classes, such as assigning higher weights to the negative reviews (minority class) or using the 'balanced' option, we can ensure that our model gives equal consideration to all classes during training. This approach can lead to improved accuracy, precision, recall, and F1-score, particularly for the minority class. Therefore, adjusting the class weights based on the class frequencies in the training data can significantly enhance the model's ability to make accurate predictions on imbalanced datasets.\n",
    "\n",
    "On the other side, despite achieving high accuracy on both the training and test sets, there is a risk of overfitting, especially given the complexity of our model. To further optimize its performance, tuning the Regularization Parameter (C) allows for balancing between fitting the training data well and preventing overfitting. By selecting an appropriate value for C, we can control the regularization strength, ensuring that the model generalizes well to unseen data while maintaining an optimal balance between model complexity and performance. This becomes particularly crucial in logistic regression models with a rich vocabulary and a large number of features, where regularization becomes essential to prevent overfitting.\n",
    "\n",
    "Therefore, we will experiment our Logistic Regression with CountVectorizer with both hyperparameter and evaluate their performance metrics such as accuracy, precision, recall, and F1-score to determine which one results in the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdeb260-8a87-4104-88ea-d9c7f76bb745",
   "metadata": {},
   "source": [
    "Logistic Regression with CountVectorizer with Class Weight hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e7f4372-3e6e-4cf7-bbe4-dc37697231b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 94.00%\n",
      "Test Accuracy: 89.34%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      5909\n",
      "           1       0.92      0.89      0.90      7561\n",
      "\n",
      "    accuracy                           0.89     13470\n",
      "   macro avg       0.89      0.89      0.89     13470\n",
      "weighted avg       0.89      0.89      0.89     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We train Logistic Regression classifier with Classweight hyperparameter\n",
    "logistic_regression_classifiercw = LogisticRegression(max_iter=1000, class_weight='balanced') \n",
    "lr_modelcw = logistic_regression_classifiercw.fit(X_train_counts, y_train)\n",
    "\n",
    "#We make predictions on the training data\n",
    "y_train_predcw = logistic_regression_classifiercw.predict(X_train_counts)\n",
    "\n",
    "#We then evaluate performance on the training set\n",
    "train_accuracycw = accuracy_score(y_train, y_train_predcw) * 100\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracycw))\n",
    "\n",
    "#We make predictions on the test data\n",
    "y_test_predcw = logistic_regression_classifiercw.predict(X_test_counts)\n",
    "\n",
    "#Then, we evaluate performance on the test set\n",
    "test_accuracycw = accuracy_score(y_test, y_test_predcw) * 100\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracycw))\n",
    "\n",
    "#Finally, we print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predcw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f50390-8d04-4a90-a505-fb5807500764",
   "metadata": {},
   "source": [
    "##### Code explanation:\n",
    "\n",
    "The above code is identical to the one used in Q3 for text classification using a Logistic Regression classifier. However, it includes an additional step where the logistic regression classifier is initialized with the class_weight parameter set to 'balanced', as discussed in Q4a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550402f-e7e8-4c34-bc70-d07f225d4839",
   "metadata": {},
   "source": [
    "##### Discussion:\n",
    "\n",
    "The above results provide insights into the performance of our Logistic Regression with CountVectorizer with Class Weight hyperparameter. Upon examination, we notice that when employing balanced class weights, our model achieves a training accuracy of 94% and test accuracy of 89.34%. Comparing these results to our previous implementation without the class weight hyperparameter (93.98% and 89.29%, respectively), we note a slight improvement in both training and test accuracies. Moreover, a closer look at our classification report reveals reasonably high values for precision and recall across both classes (0 and 1). Specifically, we observe precision values of 0.87 for class 0 and 0.92 for class 1, along with recall values of 0.89 for class 0 and 0.89 for class 1. These metrics represent a slight improvment over our model that did not incorporate the class weight hyperparameter.\n",
    "\n",
    "Note 5: Please note that prior to developing the above code, we conducted an evaluation of our accuracy using different class weights. Specifically, we utilized the following parameter grid: param_grid = {'class_weight': [{0: 1.0, 1: 1.0}, {0: 1.0, 1: 2.0}, {0: 1.0, 1: 5.0}]}. The results of this evaluation revealed training accuracies of 93.98%, 92.29%, and 88.43%, and test accuracies of 89.29%, 88.00%, and 84.95%, respectively, for the corresponding class weights. Upon analysis, we determined that the best-performing class weight was {0: 1.0, 1: 1.0}, which yielded accuracy results similar to those obtained in Q3. Consequently, we have opted to automatically balance our class weight as discussed in Q4a, resulting in a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436b54e-bef2-431a-ba5d-1d1e987f118a",
   "metadata": {},
   "source": [
    "Logistic Regression with CountVectorizer with Regularization (C) hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "803939ae-4744-407e-8680-1e1adc96392e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 93.98%\n",
      "Test Accuracy: 89.29%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      5909\n",
      "           1       0.90      0.91      0.91      7561\n",
      "\n",
      "    accuracy                           0.89     13470\n",
      "   macro avg       0.89      0.89      0.89     13470\n",
      "weighted avg       0.89      0.89      0.89     13470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We train Logistic Regression classifier with Regularization Parameter (C)\n",
    "logistic_regression_classifierc = LogisticRegression(max_iter=1000, C=1.0) \n",
    "lr_modelc = logistic_regression_classifierc.fit(X_train_counts, y_train)\n",
    "\n",
    "#We make predictions on the training data\n",
    "y_train_predc = logistic_regression_classifierc.predict(X_train_counts)\n",
    "\n",
    "#We then evaluate performance on the training set\n",
    "train_accuracyc = accuracy_score(y_train, y_train_predc) * 100\n",
    "print(\"Training Accuracy: {:.2f}%\".format(train_accuracyc))\n",
    "\n",
    "#We make predictions on the test data\n",
    "y_test_predc = logistic_regression_classifierc.predict(X_test_counts)\n",
    "\n",
    "#Then, we evaluate performance on the test set\n",
    "test_accuracyc = accuracy_score(y_test, y_test_predc) * 100\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracyc))\n",
    "\n",
    "#Finally, we print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffc52c-e2f6-45ef-8058-8435467080c1",
   "metadata": {},
   "source": [
    "##### Code explanation:\n",
    "\n",
    "The above code is identical to the one used in Q3 for text classification using a Logistic Regression classifier. However, it includes an additional step where the logistic regression classifier is initialized with the regularization parameter C is set to 1.0 by default, as discussed in Q4a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cde87-c95a-45b3-93d3-c25ccf88e21e",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "The above results provide insights into the performance of our Logistic Regression with CountVectorizer and the Regularization C hyperparameter. We notice that with our default C value (1.0), our model achieved a training accuracy of 93.98% and a test accuracy of 89.29%.  Upon comparison with our previous implementation in Q3, we observe that the introduction of the Regularization Parameter did not significantly alter the model's performance metrics, as the training and test accuracies remained similar. Furthermore, upon examining the classification report, we find reasonably high values for precision and recall for both classes (0 and 1), with precision scores of 0.89 for class 0 and 0.90 for class 1, as well as recall scores of 0.87 for class 0 and 0.91 for class 1, respectively. Interestingly, these results closely mirror those obtained from our logistic regression model in Q3.\n",
    "\n",
    "Note 6: Please note that prior to developing the above code, we conducted an evaluation of our accuracy using different C values (0.25, 0.75 and 2.0). The results of this evaluation revealed training accuracies of 90.99%, 93.48%, and 94.90%, and test accuracies of 87.25%, 89.03%, and 89.83%, respectively, for the corresponding C values. We concluded that as the C value increases, our model's accuracy tends to improve. However, when C value is large, it indicates weak regularization. However, it is important to note that higher C values indicate weaker regularization. This means that the model places more emphasis on fitting the training data closely, which could potentially lead to lower bias but higher variance. Consequently, after considering the trade-offs between bias and variance, we have opted to use the default C value (1.0) in our logistic regression model. This decision allows to have a balance between model complexity and generalization performance, aiming to achieve robust performance on unseen data while minimizing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9185483-631c-4001-92ab-f2e3b11497ef",
   "metadata": {},
   "source": [
    "#### c) Report the best hyperparameter values that result in the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85c2dd-decf-4c41-8f63-f516c2b01a00",
   "metadata": {},
   "source": [
    "Based on the above evaluation, we observed that utilizing the class weight hyperparameter resulted in higher accuracy in both training (94%) and test (89.34%) datasets. This performance was notably better compared to using the Regularization Parameter (C), which yielded similar results to our original logistic regression model with CountVectorizer. Therefore, we concluded that incorporating the class weight hyperparameter is essential for our final model due to its overall superior training and test accuracy. This enhancement is particularly crucial given the imbalance in our dataset, where positive reviews outnumber negative reviews. This ensures equitable consideration of all classes during training, leading to improved accuracy, precision, recall, and F1-score, especially for the minority class. By adjusting class weights based on the class frequencies in the training data, the model's predictive capability is significantly enhanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617fa20-3048-4417-80f4-f7a092300fb0",
   "metadata": {},
   "source": [
    "## V - Model deployment (20 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471180e-e86d-46dc-a4d3-7d0cf71f7c16",
   "metadata": {},
   "source": [
    "#### a) Deploy the model that achieved the best accuracy to an evaluation data set so that it can be used to predict the sentiment of reviews in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643aa5b-ac74-4c0e-8a91-18797eb825f4",
   "metadata": {},
   "source": [
    "Before deploying the model that achieved the best accuracy to an evaluation dataset, we first save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2dcaaf-c3f0-40e9-b307-fc361e569276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved successfully with accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "#We first convert text data into numerical feature vectors using CountVectorizer\n",
    "X_train_transformed = vectorizer_lr.fit_transform(X_train)\n",
    "\n",
    "#We initialize the best model (already known)\n",
    "best_model = logistic_regression_classifiercw\n",
    "\n",
    "#We fit the best model\n",
    "best_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "#We save the accuracy along with the best model\n",
    "accuracy = best_model.score(X_train_transformed, y_train) * 100\n",
    "\n",
    "best_model_with_accuracy = {\n",
    "    'model': best_model,\n",
    "    'accuracy': accuracy\n",
    "}\n",
    "\n",
    "#Finally, we save the best model along with its accuracy\n",
    "best_model_path = \"best_model_with_accuracy.pkl\"\n",
    "with open(best_model_path, \"wb\") as model_file:\n",
    "    pickle.dump(best_model_with_accuracy, model_file)\n",
    "\n",
    "print(\"Best model saved successfully with accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b09a3c-2b0a-4ec2-b26e-0e7c535a88ad",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code aims to save our model that achieved the best accuracy. It first converts text data into numerical feature vectors using CountVectorizer, then it initializes the best model, which is already known, as the logistic regression classifier with balanced class weights. Once done, it fits the best model on the transformed training data, calculates its accuracy, and save both as a dictionary containing the model itself and its accuracy. Finally, it saves the latter dictionary to a file using pickle and prints a message confirming that the best model has been successfully saved along with its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad068d52-5b28-4358-b743-6f30d29b49e3",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "Our best model has been successfully saved with an accuracy of 94.00%, which matches the accuracy previously found in Q4. Now that it is saved, we will proceed to deploy it on a new dataset named \"dev.csv\" for real-time sentiment prediction of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36adfe42-1bfb-4ef5-b940-277d24470914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 81.08%\n",
      "Predictions added to dev dataset and saved to: dev_with_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#We assume 'sentence' column contains the text data and 'label' column contains the labels\n",
    "X_dev = dev_df['sentence'].tolist()\n",
    "y_dev = dev_df['label'].tolist()\n",
    "\n",
    "#We load the best model\n",
    "best_model_path = \"best_model_with_accuracy.pkl\"\n",
    "with open(best_model_path, \"rb\") as model_file:\n",
    "    best_model_with_accuracy = pickle.load(model_file)\n",
    "\n",
    "best_model = best_model_with_accuracy['model']\n",
    "\n",
    "#We then transform validation data using the same CountVectorizer\n",
    "X_dev_transformed = vectorizer_lr.transform(X_dev)\n",
    "\n",
    "#We predict the sentiment of reviews in real-time\n",
    "predictions = best_model.predict(X_dev_transformed)\n",
    "\n",
    "#We calculate our validation accuracy\n",
    "validation_accuracy = accuracy_score(y_dev, predictions) * 100\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(validation_accuracy))\n",
    "\n",
    "#We add predictions to the dev dataset\n",
    "dev_df['predicted_label'] = predictions\n",
    "\n",
    "#Finally, we save the dev dataset with predictions\n",
    "dev_df_with_predictions_path = \"dev_with_predictions.csv\"\n",
    "dev_df.to_csv(dev_df_with_predictions_path, index=False)\n",
    "\n",
    "print(\"Predictions added to dev dataset and saved to:\", dev_df_with_predictions_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4500483-0eb3-4c35-a7f1-fa8136bd832b",
   "metadata": {},
   "source": [
    "##### Code explanation: \n",
    "\n",
    "The above code aims to apply our pre-trained model to predict the sentiment of movie reviews in real-time on our evaluation dataset \"dev.csv\" To do that, our code assumes that the 'sentence' column in the dev_df dataframe contains the text data (movie reviews), and the 'label' column contains the corresponding labels (please refer to Q1). Then, it extracts the 'sentence' column as input features (X_dev) and the 'label' column as target labels (y_dev) from the validation dataset. Once done, it loads our best model, along with its accuracy, from the \"best_model_with_accuracy.pkl\" file using pickle and transforms the text data (X_dev) using the same CountVectorizer that was used during training to predict the sentiment of reviews in real-time based on the transformed validation data (dev_df). Subsequently, it calculates the accuracy of the predictions compared to the actual labels in the validation dataset using the accuracy_score function from sklearn.metrics. Finally, it adds the predicted labels as a new column ('predicted_label') to the original dev_df DataFrame, saves the modified dev_data DataFrame, now including predictions, to a new CSV file named \"dev_with_predictions.csv\", prints the validation accuracy and confirms the file path where the dataset with predictions has been saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba9ec0-df47-45d9-acb4-ee2c107284f8",
   "metadata": {},
   "source": [
    "##### Discussion: \n",
    "\n",
    "A validation accuracy of 81.08% signifies that our model correctly predicted the sentiment (positive or negative) for the majority of reviews in our validation dataset (dev.csv). This indicates a strong performance on the validation data, suggesting that the model has effectively learned patterns from the training data and can generalize well to new, unseen text samples. It implies that the model is effective in discerning between positive and negative sentiments in movie reviews.\n",
    "However, it is essential to contextualize our validation accuracy. To do that, we separately calculated a baseline accuracy for our prediction model, resulting in a baseline accuracy of 50.92%. Comparing our model's accuracy to this baseline is insightful. Indeed, in our case, our model's accuracy significantly exceeds the baseline accuracy, which suggests that our model is learning meaningful patterns beyond random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ce2c6-1d4a-4f29-8d69-2b947f629183",
   "metadata": {},
   "source": [
    "#### b) Discuss the ethical implications of your solution, the scientific integrity of your design, and potential unintended consequences of your analysis more broadly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc2e16-aa87-4860-b853-35268a2e25fa",
   "metadata": {},
   "source": [
    "Despite achieving high accuracy on our evaluation dataset, it is crucial to maintain scientific integrity. Ensuring the integrity of sentiment analysis models involves utilizing balanced and diverse datasets of movie reviews. This entails curating datasets that encompass various genres, demographics, and cultural backgrounds to enhance the model's robustness and generalizability. Additionally, the labels assigned to movie reviews should accurately reflect the sentiments expressed in the text. Solely relying on automated sentiment analysis without human oversight can lead to erroneous conclusions, particularly in complex or nuanced scenarios, potentially causing users to misinterpret the sentiment analysis results and misjudge the content of movie reviews. Conversely, human annotators may introduce subjectivity or inconsistency, emphasizing the importance of validation and quality control measures in dataset annotation.\n",
    "\n",
    "Moreover, movie reviews often encapsulate personal opinions and sentiments, necessitating responsible data handling to ensure individuals' privacy rights are respected. Users should be informed about how their data is utilized for sentiment analysis, along with transparency regarding the model's limitations, biases, and potential inaccuracies.\n",
    "\n",
    "Furthermore, sentiment analysis models trained on biased reviews may perpetuate stereotypes or reinforce unequal representations in the film industry. It is crucial to ensure that sentiment analysis models do not exhibit bias towards certain groups or demographics, as biased models can lead to unfair treatment or discrimination. Additionally, biased sentiment analysis may favor mainstream or popular films over independent productions, contributing to homogeneity in the film industry and limiting opportunities for underrepresented filmmakers or genres. Cultural nuances and context in movie reviews must be considered to prevent misclassification and uphold the analysis's credibility. Indeed, sentiment analysis algorithms are frequently employed in recommendation systems to suggest movies to users. Biased or inaccurate sentiment analysis may result in skewed recommendations, restricting users' exposure to diverse perspectives or genres. If sentiment analysis algorithms have the potential to influence users' movie choices or perceptions, they may inadvertently shape audience preferences and cultural norms, impacting film consumption patterns and audience reception of diverse cinematic experiences. Negative reviews can also significantly impact filmmakers' careers and movie sales; thus, ethical sentiment analysis should provide fair and constructive feedback while avoiding unwarranted criticism or defamation.\n",
    "\n",
    "To address these concerns, regular audits and bias assessments of sentiment analysis models are essential to identify and rectify biases or inaccuracies. Implementing mechanisms for user feedback and oversight can help in correcting errors and enhancing the model's performance over time. Moreover, providing clear explanations of how the sentiment analysis system operates, including its limitations and potential biases, is crucial. Finally, considering the broader societal and cultural contexts in which the sentiment analysis model operates and adapting the approach accordingly is imperative.\n",
    "\n",
    "To conclude, ethical sentiment analysis of movie reviews requires careful consideration of biases, data quality, and potential consequences on filmmakers, audiences, and industry dynamics. By prioritizing fairness, transparency, and cultural sensitivity, sentiment analysis can contribute positively to film criticism, audience engagement, and the broader cultural discourse surrounding cinema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d29c83-3e5a-48ac-ab0f-72369bf212c8",
   "metadata": {},
   "source": [
    "### References: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23202fb9-890a-4359-9ce8-c554a04863f6",
   "metadata": {},
   "source": [
    "1) Avila, J. (2023) Regularization: Pros and cons., LinkedIn. Available at: https://www.linkedin.com/pulse/regularization-pros-cons-johanna-avila (Accessed: 12 April 2024).\n",
    "\n",
    "2) Bele, R. (2023) Naive Bayes theorem in machine learning, LinkedIn. Available at: https://www.linkedin.com/pulse/naive-bayes-theorem-machine-learning-rohit-bele#:~:text=It%20is%20computationally%20efficient%2C%20making,it%20effective%20for%20text%20classification.&text=Despite%20its%20advantages%2C%20the%20Naive,hold%20true%20in%20all%20scenarios. (Accessed: 12 April 2024).\n",
    "\n",
    "3) Bengfort, B., Bilbro, R. and Ojeda, T. (no date) Applied text analysis with python, O’Reilly Online Learning. Available at: https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html (Accessed: 12 April 2024).\n",
    "\n",
    "4) Bhor, Y. (2024) Guide for building an end-to-end logistic regression model, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2021/09/guide-for-building-an-end-to-end-logistic-regression-model/ (Accessed: 12 April 2024).\n",
    "\n",
    "5) Brownlee, J. (2020) Tune hyperparameters for Classification Machine Learning Algorithms, MachineLearningMastery.com. Available at: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/ (Accessed: 12 April 2024).\n",
    "\n",
    "6) Canuma, P. (2023) How to deal with imbalanced classification and Regression Data, neptune.ai. Available at: https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data (Accessed: 12 April 2024).\n",
    "\n",
    "7) Ganesan, K. (2020) Hashingvectorizer vs. Countvectorizer, Kavita Ganesan, PhD. Available at: https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#:~:text=Scikit%2Dlearn’s%20CountVectorizer%20is%20used,feature%20representation%20module%20for%20text. (Accessed: 12 April 2024).\n",
    "\n",
    "8) GfG (2022) Using countvectorizer to extracting features from text, GeeksforGeeks. Available at: https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/ (Accessed: 12 April 2024).\n",
    "\n",
    "9) GfG (2023) Advantages and disadvantages of logistic regression, GeeksforGeeks. Available at: https://www.geeksforgeeks.org/advantages-and-disadvantages-of-logistic-regression/ (Accessed: 12 April 2024).\n",
    "\n",
    "10) Hyperskill (n.d) TF-IDF: Count-based text representations: Text representation: NLP: Data science: Computer science. Available at: https://hyperskill.org/learn/step/31428 (Accessed: 12 April 2024).\n",
    "\n",
    "11) javatpoint (no date) Logistic regression in machine learning - www.javatpoint.com. Available at: https://www.javatpoint.com/logistic-regression-in-machine-learning (Accessed: 12 April 2024).\n",
    "\n",
    "12) Kaggle (no date) Advantages and disadvantages of logistic regression.. Available at: https://www.kaggle.com/discussions/general/352871 (Accessed: 12 April 2024).\n",
    "\n",
    "13) KDnuggets (no date). WTF is regularization and what is it for? Available at: https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for#:~:text=As%20C%20increases%20to%200.01,learn%20better%20from%20the%20data. (Accessed: 12 April 2024).\n",
    "\n",
    "14) Matplotlib documentation (2024) Pyplot tutorial. Available at: https://matplotlib.org/2.0.2/users/pyplot_tutorial.html (Accessed: 29 February 2024).\n",
    "\n",
    "15) Ninja, N. (2024) TF-IDF: Weighing importance in text, Let’s Data Science. Available at: https://letsdatascience.com/tf-idf/ (Accessed: 12 April 2024).\n",
    "\n",
    "16) Otten, N.V. (2023) TF-IDF made simple & how to get started in python tutorial, Spot Intelligence. Available at: https://spotintelligence.com/2022/11/28/tf-idf/ (Accessed: 12 April 2024).\n",
    "\n",
    "17) Pandas documentation (2024) Pandas.dataframe.replace#. Available at: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html (Accessed: 29 February 2024).\n",
    "\n",
    "18) Person (2021) Understanding TF-IDF for Machine Learning, Capital One. Available at: https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/ (Accessed: 12 April 2024).\n",
    "\n",
    "19) Python documentation (2024) CSV - csv file reading and writing . Available at: https://docs.python.org/3/library/csv.html (Accessed: 29 February 2024).\n",
    "\n",
    "20) Python documentation (2024) How to fetch internet resources using the urllib package. Available at: https://docs.python.org/3/howto/urllib2.html (Accessed: 29 February 2024).\n",
    "\n",
    "21) Python documentation (2024) OS - miscellaneous operating system interfaces . Available at: https://docs.python.org/3/library/os.html (Accessed: 29 February 2024).\n",
    "\n",
    "22) Python documentation (2024) Pickle - python object serialization. Available at: https://docs.python.org/3/library/pickle.html (Accessed: 12 April 2024). \n",
    "\n",
    "23) Python documentation (2024) Statistics - mathematical statistics functions. Available at: https://docs.python.org/3/library/statistics.html (Accessed: 29 February 2024).\n",
    "\n",
    "24) Python documentation (2024) Time - Time Access and conversions. Available at: https://docs.python.org/3/library/time.html (Accessed: 29 February 2024).\n",
    "\n",
    "25) Ray, S. (2024) Naive Bayes classifier explained: Applications and practice problems of naive Bayes classifier, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/ (Accessed: 12 April 2024).\n",
    "\n",
    "26) Real Python (2023) Python’s zipfile: Manipulate your zip files efficiently, Real Python. Available at: https://realpython.com/python-zipfile/ (Accessed: 12 April 2024).\n",
    "\n",
    "27) RE (2024) - regular expression operations (no date) Python documentation. Available at: https://docs.python.org/3/library/re.html (Accessed: 12 April 2024).\n",
    "\n",
    "\n",
    "28) scikit (2024) Sklearn.feature_extraction.text.CountVectorizer. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html (Accessed: 12 April 2024).\n",
    "\n",
    "29) scikit (2024) Sklearn.feature_extraction.text.TfidfVectorizer. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html (Accessed: 12 April 2024).\n",
    "\n",
    "30) scikit (2024) Sklearn.linear_model.logisticregression. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html (Accessed: 12 April 2024).\n",
    "\n",
    "31) scikit (2024) Sklearn.metrics.accuracy_score. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html (Accessed: 12 April 2024).\n",
    "\n",
    "32) scikit (2024) Sklearn.metrics.classification_report. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html (Accessed: 12 April 2024).\n",
    "\n",
    "33) scikit (2024) Sklearn.model_selection.train_test_split. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html (Accessed: 12 April 2024).\n",
    "\n",
    "34) scikit (2024) Sklearn.naive_bayes.multinomialnb. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html (Accessed: 12 April 2024).\n",
    "\n",
    "35) scikit (2024) 1.1. Linear Models. Available at: https://scikit-learn.org/stable/modules/linear_model.html (Accessed: 12 April 2024).\n",
    "\n",
    "36) Seaborn documentation (2024) An introduction to seaborn. Available at: https://seaborn.pydata.org/tutorial/introduction.html (Accessed: 29 February 2024). \n",
    "\n",
    "37) SitePoint (2024) Making HTTP requests in Node.js. Available at: https://www.sitepoint.com/making-http-requests-in-node-js/ (Accessed: 12 April 2024).\n",
    "\n",
    "38) TensorFlow (no date) Classification on imbalanced data  :  Tensorflow Core. Available at: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data (Accessed: 12 April 2024).\n",
    "\n",
    "39) Turing (2022) Naive Bayes algorithm in ML: Simplifying classification problems, Naive Bayes Algorithm in ML: Simplifying Classification Problems. Available at: https://www.turing.com/kb/an-introduction-to-naive-bayes-algorithm-for-beginners (Accessed: 12 April 2024). \n",
    "\n",
    "40) upGrad blog (no date)  Naive Bayes explained: Function, Advantages & disadvantages, applications in 2023. Available at: https://www.upgrad.com/blog/naive-bayes-explained/ (Accessed: 12 April 2024). \n",
    "\n",
    "41) W3 School (2024) Pandas Getting Started. Available at: https://www.w3schools.com/python/pandas/pandas_getting_started.asp (Accessed: 29 February 2024). \n",
    "\n",
    "42) W3 School (2024) NumPy getting started. Available at: https://www.w3schools.com/python/numpy/numpy_getting_started.asp (Accessed: 29 February 2024). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
